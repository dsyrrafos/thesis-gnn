{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Relations Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTroch Version 1.10.2\n",
      "GPU Available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print('PyTroch Version', torch.__version__)\n",
    "print('GPU Available:', torch.cuda.is_available())\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Key             | Value                    |\n",
      "|-----------------|--------------------------|\n",
      "| OS              | posix                    |\n",
      "| Platform        | Linux                    |\n",
      "| Release         | 5.15.0-41-generic        |\n",
      "| Time            | Sun Jul 31 00:46:46 2022 |\n",
      "| Python          | 3.9.7                    |\n",
      "| PyKEEN          | 1.7.0                    |\n",
      "| PyKEEN Hash     | UNHASHED                 |\n",
      "| PyKEEN Branch   |                          |\n",
      "| PyTorch         | 1.10.2                   |\n",
      "| CUDA Available? | false                    |\n",
      "| CUDA Version    | N/A                      |\n",
      "| cuDNN Version   | N/A                      |\n"
     ]
    }
   ],
   "source": [
    "import pykeen\n",
    "pykeen.env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'imdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: IMDB()\n",
      "Number of graphs: 1\n",
      "HeteroData(\n",
      "  \u001b[1mmovie\u001b[0m={\n",
      "    x=[4278, 3066],\n",
      "    y=[4278],\n",
      "    train_mask=[4278],\n",
      "    val_mask=[4278],\n",
      "    test_mask=[4278]\n",
      "  },\n",
      "  \u001b[1mdirector\u001b[0m={ x=[2081, 3066] },\n",
      "  \u001b[1mactor\u001b[0m={ x=[5257, 3066] },\n",
      "  \u001b[1m(movie, to, director)\u001b[0m={ edge_index=[2, 4278] },\n",
      "  \u001b[1m(movie, to, actor)\u001b[0m={ edge_index=[2, 12828] },\n",
      "  \u001b[1m(director, to, movie)\u001b[0m={ edge_index=[2, 4278] },\n",
      "  \u001b[1m(actor, to, movie)\u001b[0m={ edge_index=[2, 12828] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if dataset_name == 'dblp':\n",
    "    from torch_geometric.datasets import DBLP\n",
    "    dataset = DBLP(root='./data/dblp')\n",
    "elif dataset_name == 'imdb':\n",
    "    from torch_geometric.datasets import IMDB\n",
    "    dataset = IMDB(root='./data/imdb')\n",
    "\n",
    "print('Dataset:', dataset)\n",
    "\n",
    "print('Number of graphs:', len(dataset))\n",
    "\n",
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['movie', 'director', 'actor'],\n",
       " [('movie', 'to', 'director'),\n",
       "  ('movie', 'to', 'actor'),\n",
       "  ('director', 'to', 'movie'),\n",
       "  ('actor', 'to', 'movie')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movie'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.metadata()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie is the main entity\n",
      "['director', 'actor'] are the rest entities\n"
     ]
    }
   ],
   "source": [
    "main_entity = data.metadata()[0][0]\n",
    "rest_entities = data.metadata()[0][1:]\n",
    "print(main_entity, 'is the main entity')\n",
    "print(rest_entities, 'are the rest entities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 3\n",
      "Classes: tensor([0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(data[main_entity].y.unique())\n",
    "print('Number of classes:', num_classes)\n",
    "print('Classes:', data[main_entity].y.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change split ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to change split ratio into 20/10/70 for train/val/test with respect to class support proportion in each test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(400)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[main_entity].train_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4278"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[main_entity].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1135, 1584, 1559]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_support = torch.bincount(data[main_entity].y).tolist()\n",
    "class_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2653108929406265, 0.3702664796633941, 0.3644226273959794]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = [x/sum(class_support) for x in class_support]\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.2\n",
    "VAL_SIZE = 0.1\n",
    "\n",
    "rand_sample = np.random.rand(len(data[main_entity].x))\n",
    "\n",
    "train_msk = rand_sample < TRAIN_SIZE\n",
    "val_msk = (rand_sample < TRAIN_SIZE + VAL_SIZE) & (rand_sample >= TRAIN_SIZE)\n",
    "test_msk = rand_sample >= TRAIN_SIZE + VAL_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SIZE 786\n",
      "VAL SIZE 433\n",
      "TEST SIZE 2838\n"
     ]
    }
   ],
   "source": [
    "print('TRAIN SIZE', sum(train_msk))\n",
    "print('VAL SIZE', sum(val_msk))\n",
    "print('TEST SIZE', sum(test_msk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_msk = torch.from_numpy(train_msk)\n",
    "val_msk = torch.from_numpy(val_msk)\n",
    "test_msk = torch.from_numpy(test_msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(400)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[main_entity].train_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(786)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_msk.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[main_entity].train_mask = train_msk\n",
    "data[main_entity].val_mask = val_msk\n",
    "data[main_entity].test_mask = test_msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(786)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[main_entity].train_mask.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TransR embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating triples\n",
    "\n",
    "We want to create a (n,3)-tensor where each row will be a (node_id, relation_id, node_id) triple. We have 3 types of nodes: movie, director, actor and 4 types of realtions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranges of each node type\n",
      "Movie: 0 - 4277\n",
      "Director: 4278 - 6358\n",
      "Actor: 6359 11615\n"
     ]
    }
   ],
   "source": [
    "print('Ranges of each node type')\n",
    "print('Movie:',0,'-',data['movie'].x.size()[0]-1)\n",
    "print('Director:', data['movie'].x.size()[0], '-', data['movie'].x.size()[0]+data['director'].x.size()[0]-1)\n",
    "print('Actor:', data['movie'].x.size()[0]+data['director'].x.size()[0], data['movie'].x.size()[0]+data['director'].x.size()[0]+data['actor'].x.size()[0]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranges of each node type\n",
      "Main Entity\n",
      "author : 0 - 4056\n",
      "Rest Entities\n",
      "paper : 4057 - 18384\n",
      "term : 18385 - 26107\n",
      "conference : 26108 - 26127\n"
     ]
    }
   ],
   "source": [
    "print('Ranges of each node type')\n",
    "\n",
    "# Main Entity\n",
    "print(\"Main Entity\")\n",
    "print(main_entity,':',0,'-',data[main_entity].x.size()[0]-1)\n",
    "\n",
    "# Rest entities\n",
    "print(\"Rest Entities\")\n",
    "start = data[main_entity].x.size()[0]\n",
    "for entity in rest_entities:\n",
    "    try:\n",
    "        end = start + data[entity].x.size()[0]-1\n",
    "    except AttributeError:\n",
    "        end = start + data[entity].num_nodes-1\n",
    "    print(entity, ':', start, '-', end)\n",
    "    start = end + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reindex tails in `movie_to_director` and `movie_to_actor` relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['author', 'paper', 'term', 'conference'],\n",
       " [('author', 'to', 'paper'),\n",
       "  ('paper', 'to', 'author'),\n",
       "  ('paper', 'to', 'term'),\n",
       "  ('paper', 'to', 'conference'),\n",
       "  ('term', 'to', 'paper'),\n",
       "  ('conference', 'to', 'paper')])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "main_entity_size = data[main_entity].x.size()[0]\n",
    "current_size = main_entity_size\n",
    "for entity in rest_entities:\n",
    "    try:\n",
    "        entity_size = data[entity].x.size()[0]\n",
    "    except AttributeError:\n",
    "        entity_size = data[entity].num_nodes\n",
    "    offset_entity = torch.tensor([[0],[current_size]])\n",
    "    offset_entity = offset_entity.tile(1, data[('movie', 'to', 'director')].edge_index.size()[1])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4278 2081\n",
      "torch.Size([2, 4278]) torch.Size([2, 12828])\n"
     ]
    }
   ],
   "source": [
    "movie_size = data['movie'].x.size()[0]\n",
    "director_size = data['director'].x.size()[0]\n",
    "print(movie_size, director_size)\n",
    "movie_size = data['movie'].x.size()[0]\n",
    "director_size = data['director'].x.size()[0]\n",
    "offset_director = torch.tensor([[0],[movie_size]])\n",
    "offset_director = offset_director.tile(1, data[('movie', 'to', 'director')].edge_index.size()[1])\n",
    "movie_to_director = data[('movie', 'to', 'director')].edge_index + offset_director\n",
    "\n",
    "offset_actor = torch.tensor([[0],[movie_size + director_size]])\n",
    "offset_actor = offset_actor.tile(1, data[('movie', 'to', 'actor')].edge_index.size()[1])\n",
    "movie_to_actor = data[('movie', 'to', 'actor')].edge_index + offset_actor\n",
    "print(movie_to_director.size(), movie_to_actor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `movie-to-actor`: 0\n",
    "- `movie-to-director`: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = torch.zeros(movie_to_actor.size()[1])\n",
    "movie_to_actor = torch.column_stack((movie_to_actor[0],pad,movie_to_actor[1]))\n",
    "pad = torch.ones(movie_to_director.size()[1])\n",
    "movie_to_director = torch.column_stack((movie_to_director[0],pad,movie_to_director[1]))\n",
    "print(movie_to_director.size(), movie_to_actor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples = torch.concat((movie_to_director, movie_to_actor))\n",
    "triples.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4057 14328 7723\n",
      "torch.Size([2, 19645]) torch.Size([2, 85810]) torch.Size([2, 14328])\n"
     ]
    }
   ],
   "source": [
    "author_size = data['author'].x.size()[0]\n",
    "paper_size = data['paper'].x.size()[0]\n",
    "term_size = data['term'].x.size()[0]\n",
    "print(author_size, paper_size, term_size)\n",
    "\n",
    "# Author - Paper\n",
    "offset_paper = torch.tensor([[0],[author_size]])\n",
    "offset_paper = offset_paper.tile(1, data[('author', 'to', 'paper')].edge_index.size()[1])\n",
    "author_to_paper = data[('author', 'to', 'paper')].edge_index + offset_paper\n",
    "\n",
    "# Paper - Term\n",
    "offset_term = torch.tensor([[0],[author_size + paper_size]])\n",
    "offset_term = offset_term.tile(1, data[('paper', 'to', 'term')].edge_index.size()[1])\n",
    "paper_to_term = data[('paper', 'to', 'term')].edge_index + offset_term\n",
    "\n",
    "# Paper - Conference\n",
    "offset_conference = torch.tensor([[0],[author_size + paper_size + term_size]])\n",
    "offset_conference = offset_conference.tile(1, data[('paper', 'to', 'conference')].edge_index.size()[1])\n",
    "paper_to_conference = data[('paper', 'to', 'conference')].edge_index + offset_conference\n",
    "\n",
    "print(author_to_paper.size(), paper_to_term.size(), paper_to_conference.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `author-to-paper`: 0\n",
    "- `paper-to-term`: 1\n",
    "- `paper-to-conference`: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19645, 3]) torch.Size([85810, 3]) torch.Size([14328, 3])\n"
     ]
    }
   ],
   "source": [
    "pad = torch.zeros(author_to_paper.size()[1])\n",
    "author_to_paper = torch.column_stack((author_to_paper[0],pad,author_to_paper[1]))\n",
    "pad = torch.ones(paper_to_term.size()[1])\n",
    "paper_to_term = torch.column_stack((paper_to_term[0],pad,paper_to_term[1]))\n",
    "pad = torch.full((paper_to_conference.size()[1],), 2)\n",
    "paper_to_conference = torch.column_stack((paper_to_conference[0],pad,paper_to_conference[1]))\n",
    "print(author_to_paper.size(), paper_to_term.size(), paper_to_conference.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17106, 3])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 1.0000e+00, 5.0670e+03],\n",
       "        [1.0000e+00, 1.0000e+00, 4.9580e+03],\n",
       "        [2.0000e+00, 1.0000e+00, 6.0350e+03],\n",
       "        ...,\n",
       "        [4.2770e+03, 0.0000e+00, 6.4590e+03],\n",
       "        [4.2770e+03, 0.0000e+00, 7.4370e+03],\n",
       "        [4.2770e+03, 0.0000e+00, 7.7980e+03]])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_ids = [i for i in range (data.num_nodes)]\n",
    "relation_ids = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples.long().type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['TransR', 'TransH', 'RotatE', 'DistMult', 'ComplEx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.utils:No cuda devices were available. The model runs on CPU\n",
      "Training epochs on cpu: 100%|██████████| 100/100 [11:20<00:00,  6.81s/epoch, loss=4.29e-5, prev_loss=4.53e-5]\n",
      "INFO:pykeen.evaluation.evaluator:Currently automatic memory optimization only supports GPUs, but you're using a CPU. Therefore, the batch_size will be set to the default value.\n",
      "INFO:pykeen.evaluation.evaluator:No evaluation batch_size provided. Setting batch_size to '32'.\n",
      "Evaluating on cpu: 100%|██████████| 17.1k/17.1k [17:54<00:00, 15.9triple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 1074.44s seconds\n",
      "WARNING:pykeen.utils:No cuda devices were available. The model runs on CPU\n",
      "Training epochs on cpu: 100%|██████████| 100/100 [03:36<00:00,  2.16s/epoch, loss=0.00273, prev_loss=0.00274]\n",
      "INFO:pykeen.evaluation.evaluator:Currently automatic memory optimization only supports GPUs, but you're using a CPU. Therefore, the batch_size will be set to the default value.\n",
      "INFO:pykeen.evaluation.evaluator:No evaluation batch_size provided. Setting batch_size to '32'.\n",
      "Evaluating on cpu: 100%|██████████| 17.1k/17.1k [07:29<00:00, 38.0triple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 449.92s seconds\n",
      "WARNING:pykeen.utils:No cuda devices were available. The model runs on CPU\n",
      "Training epochs on cpu: 100%|██████████| 100/100 [04:42<00:00,  2.83s/epoch, loss=2.19e-6, prev_loss=1.8e-6]\n",
      "INFO:pykeen.evaluation.evaluator:Currently automatic memory optimization only supports GPUs, but you're using a CPU. Therefore, the batch_size will be set to the default value.\n",
      "INFO:pykeen.evaluation.evaluator:No evaluation batch_size provided. Setting batch_size to '32'.\n",
      "Evaluating on cpu: 100%|██████████| 17.1k/17.1k [05:23<00:00, 52.9triple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 323.24s seconds\n",
      "WARNING:pykeen.utils:No cuda devices were available. The model runs on CPU\n",
      "Training epochs on cpu: 100%|██████████| 100/100 [03:15<00:00,  1.95s/epoch, loss=0.00315, prev_loss=0.00315]\n",
      "INFO:pykeen.evaluation.evaluator:Currently automatic memory optimization only supports GPUs, but you're using a CPU. Therefore, the batch_size will be set to the default value.\n",
      "INFO:pykeen.evaluation.evaluator:No evaluation batch_size provided. Setting batch_size to '32'.\n",
      "Evaluating on cpu: 100%|██████████| 17.1k/17.1k [02:25<00:00, 118triple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 145.06s seconds\n",
      "WARNING:pykeen.utils:No cuda devices were available. The model runs on CPU\n",
      "Training epochs on cpu: 100%|██████████| 100/100 [04:44<00:00,  2.84s/epoch, loss=0.000138, prev_loss=0.000144]\n",
      "INFO:pykeen.evaluation.evaluator:Currently automatic memory optimization only supports GPUs, but you're using a CPU. Therefore, the batch_size will be set to the default value.\n",
      "INFO:pykeen.evaluation.evaluator:No evaluation batch_size provided. Setting batch_size to '32'.\n",
      "Evaluating on cpu: 100%|██████████| 17.1k/17.1k [03:27<00:00, 82.3triple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 207.76s seconds\n"
     ]
    }
   ],
   "source": [
    "from pykeen.triples import CoreTriplesFactory\n",
    "from pykeen.pipeline import pipeline\n",
    "\n",
    "def train_kg_emb_model(model, triples, data, entity_ids, relation_ids):\n",
    "    # Load training data\n",
    "    num_epochs = 100\n",
    "    emb_dim = 200\n",
    "    rel_dim = 200\n",
    "    \n",
    "    if model == 'TransR':\n",
    "        model_kwargs = {\"embedding_dim\":emb_dim,\n",
    "                        \"relation_dim\": rel_dim}\n",
    "    else:\n",
    "        model_kwargs = {\"embedding_dim\":emb_dim}\n",
    "\n",
    "    training = CoreTriplesFactory(mapped_triples=triples.long(), num_entities=data.num_nodes, \n",
    "                                    num_relations=2, create_inverse_triples=False,\n",
    "                                    entity_ids=entity_ids, relation_ids=relation_ids)\n",
    "\n",
    "    result = pipeline(\n",
    "        training=training,\n",
    "        testing=training,\n",
    "        model=model,\n",
    "        random_seed=42,\n",
    "        model_kwargs=model_kwargs,\n",
    "        training_kwargs={\"num_epochs\":num_epochs},\n",
    "        #stopper='early', # early stopping arguments. You need the validation set with this.\n",
    "        #stopper_kwargs=dict(frequency=3, patience=3, relative_delta=0.002),\n",
    "        #epochs=5,  # short epochs for testing - you should go higher\n",
    "    )\n",
    "\n",
    "    # Save mode to a directory. You can load it afterwards\n",
    "    result.save_to_directory('Models_pykeen/imdb_' + model + '_ep_' + str(num_epochs) + '_dim_' + str(emb_dim))\n",
    "    return result\n",
    "\n",
    "for model in models:\n",
    "    result = train_kg_emb_model(model, triples, data, entity_ids, relation_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('author', 'to', 'paper'),\n",
       " ('paper', 'to', 'author'),\n",
       " ('paper', 'to', 'term'),\n",
       " ('paper', 'to', 'conference'),\n",
       " ('term', 'to', 'paper'),\n",
       " ('conference', 'to', 'paper')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.metadata()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SplitGCN(\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (conv): HeteroConv(num_relations=2)\n",
      "  (linears_rel): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): Linear(128, 200, bias=True)\n",
      "      (1): Linear(128, 200, bias=True)\n",
      "      (2): Linear(128, 200, bias=True)\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): Linear(128, 200, bias=True)\n",
      "      (1): Linear(128, 200, bias=True)\n",
      "      (2): Linear(128, 200, bias=True)\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): Linear(128, 200, bias=True)\n",
      "      (1): Linear(128, 200, bias=True)\n",
      "      (2): Linear(128, 200, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (rel_mlps): ModuleList(\n",
      "    (0): Linear(128, 200, bias=True)\n",
      "    (1): Linear(128, 200, bias=True)\n",
      "    (2): Linear(128, 200, bias=True)\n",
      "  )\n",
      "  (linears_clf): ModuleList(\n",
      "    (0): Linear(600, 200, bias=True)\n",
      "    (1): Linear(200, 200, bias=True)\n",
      "    (2): Linear(200, 4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "672"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "from torch_geometric.nn import Linear, HeteroConv, GCNConv, SAGEConv, GATConv\n",
    "\n",
    "class SplitGCN(torch.nn.Module):\n",
    "    def __init__(self, metadata, emb_size, dense_size, out_size, \n",
    "                    num_dense_layers, num_clf_layers, p, device='cpu', transr=None):\n",
    "\n",
    "        # TODO: Implement a function that initializes self.convs, \n",
    "        # self.bns, and self.softmax.\n",
    "        super(SplitGCN, self).__init__()\n",
    "\n",
    "        self.num_relations = int(len(metadata[1])/2)\n",
    "        self.device = device\n",
    "        self.edge_conv_dict = {}\n",
    "        self.num_dense_layers = num_dense_layers\n",
    "        self.num_clf_layers = num_clf_layers\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        for node_type in [('author', 'paper'), ('paper', 'term'), ('paper', 'conference')]:\n",
    "            self.conv = HeteroConv({\n",
    "                (node_type[0], 'to', node_type[1]): SAGEConv((-1, -1), emb_size),\n",
    "                (node_type[1], 'to', node_type[0]): SAGEConv((-1, -1), emb_size)\n",
    "            })\n",
    "            self.edge_conv_dict[node_type] = self.conv\n",
    "\n",
    "        if transr is not None:\n",
    "            self.transr = transr\n",
    "\n",
    "        self.linears_rel = nn.ModuleList()\n",
    "        for i in range (self.num_relations):\n",
    "            self.rel_mlps = nn.ModuleList()\n",
    "            try:\n",
    "                linear = Linear(emb_size + self.transr[i].size()[1], dense_size)\n",
    "            except:\n",
    "                linear = Linear(emb_size, dense_size)\n",
    "            self.rel_mlps.append(linear)\n",
    "            for i in range(num_dense_layers-1):\n",
    "                linear = Linear(emb_size, dense_size)\n",
    "                self.rel_mlps.append(linear)\n",
    "            self.linears_rel.append(self.rel_mlps)\n",
    "        \n",
    "        self.linears_clf = nn.ModuleList()\n",
    "        clflinear = Linear(self.num_relations*dense_size, dense_size)\n",
    "        self.linears_clf.append(clflinear)\n",
    "        for i in range(self.num_clf_layers - 2):\n",
    "            clflinear = Linear(dense_size, dense_size)\n",
    "\n",
    "            self.linears_clf.append(clflinear)\n",
    "        clflinear = Linear(dense_size, num_classes)\n",
    "        self.linears_clf.append(clflinear)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, transr=None):\n",
    "        # TODO: Implement a function that takes the feature tensor x and\n",
    "        # edge_index tensor adj_t and returns the output tensor as\n",
    "        # shown in the figure.\n",
    "        \n",
    "        concatenated_embs = []\n",
    "        for node_type in [('author', 'paper'), ('paper', 'term'), ('paper', 'conference')]:\n",
    "            conv = self.edge_conv_dict[node_type]\n",
    "            \n",
    "            relation = (node_type[0], 'to', node_type[1])\n",
    "            reverse = (node_type[1], 'to', node_type[0])\n",
    "\n",
    "            single_edge_index_dict = {relation: edge_index_dict[relation].to(self.device),\n",
    "                                        reverse: edge_index_dict[reverse].to(self.device)}\n",
    "            \n",
    "            single_x_dict = {relation[0]: x_dict[relation[0]].to(self.device), \n",
    "                            relation[2]: x_dict[relation[2]].to(self.device)}\n",
    "\n",
    "            single_x_dict = conv(single_x_dict, single_edge_index_dict)\n",
    "            concatenated_embs.append(single_x_dict['author'])\n",
    "\n",
    "        linear_outputs = []\n",
    "        #print(\"here\")\n",
    "        #print(len(concatenated_embs), len(self.linears_rel))\n",
    "        for i, (x, mlp) in enumerate(zip(concatenated_embs, self.linears_rel)):\n",
    "            #print(type(x))\n",
    "            #print(x.size())\n",
    "            #print(len(transr))\n",
    "            #print(transr[0].size())\n",
    "            if transr:\n",
    "                x = torch.cat((x, transr[i]), 1)\n",
    "            #print(x.size())\n",
    "            for linear in mlp:\n",
    "                #print(\"im in\")\n",
    "                #print(x.size())\n",
    "                x = linear(x)\n",
    "                x = self.relu(x)\n",
    "                x = self.dropout(x)\n",
    "            \n",
    "            #print('linear output', x.size())\n",
    "            linear_outputs.append(x)\n",
    "        \n",
    "        #print(len(linear_outputs))\n",
    "        out = torch.cat(linear_outputs, dim=-1)\n",
    "        #print('clf layer input', out.size())\n",
    "        for i in range(len(self.linears_clf) - 1):\n",
    "            out = self.linears_clf[i](out)\n",
    "            out = self.relu(out)\n",
    "            out = self.dropout(out)\n",
    "        out = self.linears_clf[-1](out)\n",
    "        #print('clf layer output', out.size())\n",
    "        return out\n",
    "\n",
    "\n",
    "model = SplitGCN(data.metadata(), 128, 200, num_classes, 3, 3, 0.2)\n",
    "print(model)\n",
    "del model\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SplitGCN(\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (conv): HeteroConv(num_relations=2)\n",
      "  (linears_rel): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): Linear(128, 200, bias=True)\n",
      "      (1): Linear(128, 200, bias=True)\n",
      "      (2): Linear(128, 200, bias=True)\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): Linear(128, 200, bias=True)\n",
      "      (1): Linear(128, 200, bias=True)\n",
      "      (2): Linear(128, 200, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (rel_mlps): ModuleList(\n",
      "    (0): Linear(128, 200, bias=True)\n",
      "    (1): Linear(128, 200, bias=True)\n",
      "    (2): Linear(128, 200, bias=True)\n",
      "  )\n",
      "  (linears_clf): ModuleList(\n",
      "    (0): Linear(400, 200, bias=True)\n",
      "    (1): Linear(200, 200, bias=True)\n",
      "    (2): Linear(200, 3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "473"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "from torch_geometric.nn import Linear, HeteroConv, GCNConv, SAGEConv, GATConv\n",
    "\n",
    "class SplitGCN(torch.nn.Module):\n",
    "    def __init__(self, metadata, emb_size, dense_size, out_size, \n",
    "                    num_dense_layers, num_clf_layers, p, device='cpu', transr=None):\n",
    "\n",
    "        # TODO: Implement a function that initializes self.convs, \n",
    "        # self.bns, and self.softmax.\n",
    "        super(SplitGCN, self).__init__()\n",
    "\n",
    "        self.num_relations = int(len(metadata[1])/2)\n",
    "        self.device = device\n",
    "        self.edge_conv_dict = {}\n",
    "        self.num_dense_layers = num_dense_layers\n",
    "        self.num_clf_layers = num_clf_layers\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        for node_type in ['actor', 'director']:\n",
    "            self.conv = HeteroConv({\n",
    "                ('movie', 'to', node_type): SAGEConv((-1, -1), emb_size),\n",
    "                (node_type, 'to', 'movie'): SAGEConv((-1, -1), emb_size)\n",
    "            })\n",
    "            self.edge_conv_dict[node_type] = self.conv\n",
    "\n",
    "        if transr is not None:\n",
    "            self.transr = transr\n",
    "\n",
    "        self.linears_rel = nn.ModuleList()\n",
    "        for i in range (self.num_relations):\n",
    "            self.rel_mlps = nn.ModuleList()\n",
    "            try:\n",
    "                linear = Linear(emb_size + self.transr[i].size()[1], dense_size)\n",
    "            except:\n",
    "                linear = Linear(emb_size, dense_size)\n",
    "            self.rel_mlps.append(linear)\n",
    "            for i in range(num_dense_layers-1):\n",
    "                linear = Linear(emb_size, dense_size)\n",
    "                self.rel_mlps.append(linear)\n",
    "            self.linears_rel.append(self.rel_mlps)\n",
    "        \n",
    "        self.linears_clf = nn.ModuleList()\n",
    "        clflinear = Linear(self.num_relations*dense_size, dense_size)\n",
    "        self.linears_clf.append(clflinear)\n",
    "        for i in range(self.num_clf_layers - 2):\n",
    "            clflinear = Linear(dense_size, dense_size)\n",
    "\n",
    "            self.linears_clf.append(clflinear)\n",
    "        clflinear = Linear(dense_size, num_classes)\n",
    "        self.linears_clf.append(clflinear)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, transr=None):\n",
    "        # TODO: Implement a function that takes the feature tensor x and\n",
    "        # edge_index tensor adj_t and returns the output tensor as\n",
    "        # shown in the figure.\n",
    "        \n",
    "        concatenated_embs = []\n",
    "        for node_type in ['actor', 'director']:\n",
    "            conv = self.edge_conv_dict[node_type]\n",
    "            \n",
    "            relation = ('movie', 'to', node_type)\n",
    "            reverse = (node_type, 'to', 'movie')\n",
    "\n",
    "            single_edge_index_dict = {relation: edge_index_dict[relation].to(self.device),\n",
    "                                        reverse: edge_index_dict[reverse].to(self.device)}\n",
    "            \n",
    "            single_x_dict = {relation[0]: x_dict[relation[0]].to(self.device), \n",
    "                            relation[2]: x_dict[relation[2]].to(self.device)}\n",
    "\n",
    "            single_x_dict = conv(single_x_dict, single_edge_index_dict)\n",
    "            concatenated_embs.append(single_x_dict['movie'])\n",
    "\n",
    "        linear_outputs = []\n",
    "        #print(\"here\")\n",
    "        #print(len(concatenated_embs), len(self.linears_rel))\n",
    "        for i, (x, mlp) in enumerate(zip(concatenated_embs, self.linears_rel)):\n",
    "            #print(type(x))\n",
    "            #print(x.size())\n",
    "            #print(len(transr))\n",
    "            #print(transr[0].size())\n",
    "            if transr:\n",
    "                x = torch.cat((x, transr[i]), 1)\n",
    "            #print(x.size())\n",
    "            for linear in mlp:\n",
    "                #print(\"im in\")\n",
    "                #print(x.size())\n",
    "                x = linear(x)\n",
    "                x = self.relu(x)\n",
    "                x = self.dropout(x)\n",
    "            \n",
    "            #print('linear output', x.size())\n",
    "            linear_outputs.append(x)\n",
    "        \n",
    "        #print(len(linear_outputs))\n",
    "        out = torch.cat(linear_outputs, dim=-1)\n",
    "        #print('clf layer input', out.size())\n",
    "        for i in range(len(self.linears_clf) - 1):\n",
    "            out = self.linears_clf[i](out)\n",
    "            out = self.relu(out)\n",
    "            out = self.dropout(out)\n",
    "        out = self.linears_clf[-1](out)\n",
    "        #print('clf layer output', out.size())\n",
    "        return out\n",
    "\n",
    "\n",
    "model = SplitGCN(data.metadata(), 128, 200, num_classes, 3, 3, 0.2)\n",
    "print(model)\n",
    "del model\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SplitGCN(\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (conv): HeteroConv(num_relations=2)\n",
      "  (linears_rel): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): Linear(128, 200, bias=True)\n",
      "      (1): Linear(128, 200, bias=True)\n",
      "      (2): Linear(128, 200, bias=True)\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): Linear(128, 200, bias=True)\n",
      "      (1): Linear(128, 200, bias=True)\n",
      "      (2): Linear(128, 200, bias=True)\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): Linear(128, 200, bias=True)\n",
      "      (1): Linear(128, 200, bias=True)\n",
      "      (2): Linear(128, 200, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (rel_mlps): ModuleList(\n",
      "    (0): Linear(128, 200, bias=True)\n",
      "    (1): Linear(128, 200, bias=True)\n",
      "    (2): Linear(128, 200, bias=True)\n",
      "  )\n",
      "  (linears_clf): ModuleList(\n",
      "    (0): Linear(600, 200, bias=True)\n",
      "    (1): Linear(200, 200, bias=True)\n",
      "    (2): Linear(200, 4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1577"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "from torch_geometric.nn import Linear, HeteroConv, GCNConv, SAGEConv, GATConv\n",
    "\n",
    "class SplitGCN(torch.nn.Module):\n",
    "    def __init__(self, metadata, emb_size, dense_size, out_size, \n",
    "                    num_dense_layers, num_clf_layers, p, device='cpu', transr=None):\n",
    "\n",
    "        # TODO: Implement a function that initializes self.convs, \n",
    "        # self.bns, and self.softmax.\n",
    "        super(SplitGCN, self).__init__()\n",
    "\n",
    "        self.num_relations = int(len(metadata[1])/2)\n",
    "        self.device = device\n",
    "        self.edge_conv_dict = {}\n",
    "        self.num_dense_layers = num_dense_layers\n",
    "        self.num_clf_layers = num_clf_layers\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "        '''\n",
    "        [('author', 'to', 'paper'),\n",
    "        ('paper', 'to', 'author'),\n",
    "        ('paper', 'to', 'term'),\n",
    "        ('paper', 'to', 'conference'),\n",
    "        ('term', 'to', 'paper'),\n",
    "        ('conference', 'to', 'paper')]\n",
    "        '''\n",
    "    \n",
    "        self.conv = HeteroConv({\n",
    "            ('author', 'to', 'paper'): SAGEConv((-1, -1), emb_size),\n",
    "            ('paper', 'to', 'author'): SAGEConv((-1, -1), emb_size)\n",
    "        })\n",
    "        self.edge_conv_dict['author'] = self.conv\n",
    "\n",
    "        self.conv = HeteroConv({\n",
    "            ('term', 'to', 'paper'): SAGEConv((-1, -1), emb_size),\n",
    "            ('paper', 'to', 'term'): SAGEConv((-1, -1), emb_size)\n",
    "        })\n",
    "        self.edge_conv_dict['term'] = self.conv\n",
    "\n",
    "        self.conv = HeteroConv({\n",
    "            ('conference', 'to', 'paper'): SAGEConv((-1, -1), emb_size),\n",
    "            ('paper', 'to', 'conference'): SAGEConv((-1, -1), emb_size)\n",
    "        })\n",
    "        self.edge_conv_dict['conference'] = self.conv\n",
    "\n",
    "\n",
    "        if transr is not None:\n",
    "            self.transr = transr\n",
    "\n",
    "        self.linears_rel = nn.ModuleList()\n",
    "        for i in range (self.num_relations):\n",
    "            self.rel_mlps = nn.ModuleList()\n",
    "            try:\n",
    "                linear = Linear(emb_size + self.transr[i].size()[1], dense_size)\n",
    "            except:\n",
    "                linear = Linear(emb_size, dense_size)\n",
    "            self.rel_mlps.append(linear)\n",
    "            for i in range(num_dense_layers-1):\n",
    "                linear = Linear(emb_size, dense_size)\n",
    "                self.rel_mlps.append(linear)\n",
    "            self.linears_rel.append(self.rel_mlps)\n",
    "        \n",
    "        self.linears_clf = nn.ModuleList()\n",
    "        clflinear = Linear(self.num_relations*dense_size, dense_size)\n",
    "        self.linears_clf.append(clflinear)\n",
    "        for i in range(self.num_clf_layers - 2):\n",
    "            clflinear = Linear(dense_size, dense_size)\n",
    "\n",
    "            self.linears_clf.append(clflinear)\n",
    "        clflinear = Linear(dense_size, num_classes)\n",
    "        self.linears_clf.append(clflinear)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, transr=None):\n",
    "        # TODO: Implement a function that takes the feature tensor x and\n",
    "        # edge_index tensor adj_t and returns the output tensor as\n",
    "        # shown in the figure.\n",
    "        \n",
    "        concatenated_embs = []\n",
    "        for node_type in ['author', 'term']:\n",
    "            conv = self.edge_conv_dict[node_type]\n",
    "            \n",
    "            relation = ('paper', 'to', node_type)\n",
    "            reverse = (node_type, 'to', 'paper')\n",
    "\n",
    "            single_edge_index_dict = {relation: edge_index_dict[relation].to(self.device),\n",
    "                                        reverse: edge_index_dict[reverse].to(self.device)}\n",
    "            \n",
    "            single_x_dict = {relation[0]: x_dict[relation[0]].to(self.device), \n",
    "                            relation[2]: x_dict[relation[2]].to(self.device)}\n",
    "\n",
    "            single_x_dict = conv(single_x_dict, single_edge_index_dict)\n",
    "            concatenated_embs.append(single_x_dict['author'])\n",
    "\n",
    "        linear_outputs = []\n",
    "        #print(\"here\")\n",
    "        #print(len(concatenated_embs), len(self.linears_rel))\n",
    "        for i, (x, mlp) in enumerate(zip(concatenated_embs, self.linears_rel)):\n",
    "            #print(type(x))\n",
    "            #print(x.size())\n",
    "            #print(len(transr))\n",
    "            #print(transr[0].size())\n",
    "            if transr:\n",
    "                print(x)\n",
    "                x = torch.cat((x, transr[i]), 1)\n",
    "            #print(x.size())\n",
    "            for linear in mlp:\n",
    "                #print(\"im in\")\n",
    "                #print(x.size())\n",
    "                x = linear(x)\n",
    "                x = self.relu(x)\n",
    "                x = self.dropout(x)\n",
    "            \n",
    "            #print('linear output', x.size())\n",
    "            linear_outputs.append(x)\n",
    "        \n",
    "        #print(len(linear_outputs))\n",
    "        out = torch.cat(linear_outputs, dim=-1)\n",
    "        #print('clf layer input', out.size())\n",
    "        for i in range(len(self.linears_clf) - 1):\n",
    "            out = self.linears_clf[i](out)\n",
    "            out = self.relu(out)\n",
    "            out = self.dropout(out)\n",
    "        out = self.linears_clf[-1](out)\n",
    "        #print('clf layer output', out.size())\n",
    "        return out\n",
    "\n",
    "\n",
    "model = SplitGCN(data.metadata(), 128, 200, num_classes, 3, 3, 0.2)\n",
    "print(model)\n",
    "del model\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, loss_fn, target_entity, transr=None):\n",
    "    # TODO: Implement a function that trains the model by \n",
    "    # using the given optimizer and loss_fn.\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(data.x_dict, data.edge_index_dict, transr)\n",
    "    mask = data[target_entity].train_mask\n",
    "    labels = data[target_entity].y[mask]\n",
    "    loss = loss_fn(outputs[mask], labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function here\n",
    "@torch.no_grad()\n",
    "def test(model, data, target_entity, transr=None, save_model_results=False):\n",
    "    # a function that tests the model by \n",
    "    # using the given split_idx and evaluator.\n",
    "    model.eval()\n",
    "\n",
    "    # The output of model on all data\n",
    "    out = model(data.x_dict, data.edge_index_dict, transr)\n",
    "    pred = out.argmax(dim=-1, keepdim=True)[:,0]\n",
    "    \n",
    "    accs = []\n",
    "    f1scores = []\n",
    "    for split in ['train_mask', 'val_mask', 'test_mask']:\n",
    "        mask = data[target_entity][split]\n",
    "        acc = (pred[mask] == data[target_entity].y[mask]).sum() / mask.sum()\n",
    "        accs.append(float(acc))\n",
    "        f1 = f1_score(data[target_entity].y[mask], pred[mask], average='macro')\n",
    "        f1scores.append(float(f1))\n",
    "        #print(pred[mask].size(), data['movie'].y[mask].size())\n",
    "        #print(pred[mask].sum(), data['movie'].y[mask].sum())\n",
    "        #print(mask.sum(), (pred[mask] == data['movie'].y[mask]).sum())\n",
    "\n",
    "    if save_model_results:\n",
    "      print (\"Saving Model Predictions\")\n",
    "\n",
    "      data = {}\n",
    "      data['y_pred'] = pred.view(-1).cpu().detach().numpy()\n",
    "\n",
    "      df = pd.DataFrame(data=data)\n",
    "      # Save locally as csv\n",
    "      df.to_csv('imdb.csv', sep=',', index=False)\n",
    "\n",
    "\n",
    "    return (accs, f1scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Evaluate with KG Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 786\n",
      "Valid size: 433\n",
      "Test size: 2838\n",
      "Total size: 4057\n"
     ]
    }
   ],
   "source": [
    "train_size = data[main_entity].train_mask.sum().item()\n",
    "val_size = data[main_entity].val_mask.sum().item()\n",
    "test_size = data[main_entity].test_mask.sum().item()\n",
    "print(\"Train size:\", train_size)\n",
    "print(\"Valid size:\", val_size)\n",
    "print(\"Test size:\", test_size)\n",
    "print(\"Total size:\", train_size + val_size + test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def run_train_eval(target_entity, data, kg_emb, seed, output_file):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    print('KG embedding model:', kg_emb, \"Seed:\", seed)\n",
    "\n",
    "    transr_model = torch.load('Models_pykeen/' + kg_emb + '/trained_model.pkl')\n",
    "\n",
    "    transr_emb_entity = transr_model.entity_representations[0](indices=None).detach().numpy()\n",
    "    transr_emb_relation = transr_model.relation_representations[0](indices=None).detach().numpy()\n",
    "    num_clf_nodes = data[data.node_types[0]].x.shape[0]\n",
    "    embs = transr_emb_relation\n",
    "\n",
    "    rel_embs_list = []\n",
    "    for i in range (embs.shape[0]):\n",
    "        rel_embs_list.append(torch.tensor(np.tile(embs[i], (num_clf_nodes, 1))))\n",
    "    print('Number of relation embeddings:', len(rel_embs_list))\n",
    "    print('Embeddings size', rel_embs_list[0].size())\n",
    "\n",
    "    # Model Parameters\n",
    "    emb_dim = 128\n",
    "    hidden_dim = 128\n",
    "    output_dim = num_classes\n",
    "    num_rel_layers = 2\n",
    "    num_clf_layers = 2\n",
    "    p = 0.2\n",
    "\n",
    "    model = SplitGCN(data.metadata(), emb_dim, hidden_dim, output_dim, num_rel_layers, num_clf_layers, p, device, rel_embs_list)        \n",
    "    # num_layers = 3\n",
    "    # model = SplitGCN(data.metadata(), emb_dim, hidden_dim, output_dim, num_layers, device, rel_embs_list)\n",
    "\n",
    "\n",
    "    model, data = model.to(device), data.to(device)\n",
    "\n",
    "    print(next(model.parameters()).device)\n",
    "\n",
    "    # Reset model parameters\n",
    "    # model.reset_parameters()\n",
    "\n",
    "    # Define hyperparameters\n",
    "    num_epochs = 300\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    loss_fn = F.cross_entropy\n",
    "\n",
    "    best_model = None\n",
    "    best_test_acc = 0\n",
    "    best_test_f1 = 0\n",
    "\n",
    "    for epoch in range(1, 1 + num_epochs):\n",
    "        # train model\n",
    "        loss = train(model, data, optimizer, loss_fn, target_entity, rel_embs_list)\n",
    "        \n",
    "        # evaluate model \n",
    "        (accuracy, f1) = test(model, data, target_entity, rel_embs_list)\n",
    "        \n",
    "        train_acc, valid_acc, test_acc = accuracy\n",
    "        _, _, test_f1 = f1\n",
    "        if test_f1 > best_test_f1:\n",
    "            best_test_f1 = test_f1\n",
    "            # best_model = copy.deepcopy(model)\n",
    "        print(f'Epoch: {epoch:02d}, '\n",
    "                f'Loss: {loss:.4f}, '\n",
    "                f'Train: {100 * train_acc:.2f}%, '\n",
    "                f'Valid: {100 * valid_acc:.2f}% '\n",
    "                f'Test: {100 * test_acc:.2f}%')\n",
    "    \n",
    "    with open(output_file, \"a\") as f:\n",
    "        f.write(kg_emb + ' best test f1 score: ' + str(best_test_f1) + '\\n')\n",
    "    \n",
    "    return model, best_test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dblp_TransR_ep_100_dim_200',\n",
       " 'dblp_TransH_ep_100_dim_200',\n",
       " 'dblp_ComplEx_ep_100_dim_200',\n",
       " 'dblp_RotatE_ep_100_dim_200',\n",
       " 'dblp_DistMult_ep_100_dim_200']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_emb_models = [] \n",
    "for model in os.listdir('./Models_pykeen'):\n",
    "    if 'ep' in model and 'dblp' in model:\n",
    "        kg_emb_models.append(model)\n",
    "kg_emb_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG embedding model: dblp_TransR_ep_100_dim_200 Seed: 0\n",
      "Number of relation embeddings: 2\n",
      "Embeddings size torch.Size([4057, 200])\n",
      "cpu\n",
      "tensor([[ 0.0119, -0.0454,  0.1339,  ..., -0.0889, -0.0362,  0.0005],\n",
      "        [ 0.1192,  0.0404,  0.0743,  ...,  0.2375,  0.1329,  0.0346],\n",
      "        [-0.0386,  0.0194,  0.2685,  ...,  0.0118,  0.0783, -0.0123],\n",
      "        ...,\n",
      "        [-0.0230,  0.0302,  0.0034,  ...,  0.0028,  0.0186,  0.0046],\n",
      "        [-0.0230,  0.0302,  0.0034,  ...,  0.0028,  0.0186,  0.0046],\n",
      "        [ 0.0501, -0.0377,  0.1515,  ..., -0.0804, -0.0108, -0.0979]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "[]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_53600/3817554697.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtest_acc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mmodel_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_train_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_entity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtest_acc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mavg_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_53600/2496313334.py\u001b[0m in \u001b[0;36mrun_train_eval\u001b[0;34m(target_entity, data, kg_emb, seed, output_file)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_entity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_embs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_53600/2710007382.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, optimizer, loss_fn, target_entity, transr)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_entity\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_entity\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis-gnn-cpu/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_53600/397253073.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_dict, edge_index_dict, transr)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;31m#print(x.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlinear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "output_file = 'scores_sage_relu_dblp.txt'\n",
    "for i, model in enumerate(kg_emb_models):\n",
    "    test_acc_list = []\n",
    "    for seed in range (5): \n",
    "        model_summary, acc = run_train_eval(main_entity, data, model, seed, output_file)\n",
    "        test_acc_list.append(acc)\n",
    "    avg_acc = sum(test_acc_list)/5\n",
    "\n",
    "    if i==0 and seed==0:\n",
    "        with open(output_file, \"a\") as f:\n",
    "            f.write(model_summary)    \n",
    "    \n",
    "    with open(output_file, \"a\") as f:\n",
    "        f.write('Average accuracy: ' + str(avg_acc) + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Evaluate without KG Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch: 01, Loss: 1.1009, Train: 66.00%, Valid: 47.50% Test: 41.09%\n",
      "Epoch: 02, Loss: 1.0828, Train: 73.75%, Valid: 51.50% Test: 43.62%\n",
      "Epoch: 03, Loss: 1.0632, Train: 74.00%, Valid: 49.00% Test: 43.47%\n",
      "Epoch: 04, Loss: 1.0405, Train: 74.75%, Valid: 49.00% Test: 43.21%\n",
      "Epoch: 05, Loss: 1.0140, Train: 75.75%, Valid: 49.25% Test: 43.04%\n",
      "Epoch: 06, Loss: 0.9834, Train: 77.00%, Valid: 49.25% Test: 43.13%\n",
      "Epoch: 07, Loss: 0.9486, Train: 80.00%, Valid: 48.50% Test: 43.42%\n",
      "Epoch: 08, Loss: 0.9097, Train: 83.25%, Valid: 49.50% Test: 43.65%\n",
      "Epoch: 09, Loss: 0.8667, Train: 85.25%, Valid: 50.00% Test: 44.02%\n",
      "Epoch: 10, Loss: 0.8199, Train: 88.50%, Valid: 50.00% Test: 44.62%\n",
      "Epoch: 11, Loss: 0.7697, Train: 90.75%, Valid: 51.00% Test: 45.37%\n",
      "Epoch: 12, Loss: 0.7166, Train: 92.00%, Valid: 51.50% Test: 46.89%\n",
      "Epoch: 13, Loss: 0.6611, Train: 93.50%, Valid: 53.25% Test: 47.90%\n",
      "Epoch: 14, Loss: 0.6040, Train: 94.50%, Valid: 54.00% Test: 48.79%\n",
      "Epoch: 15, Loss: 0.5461, Train: 95.50%, Valid: 54.00% Test: 49.94%\n",
      "Epoch: 16, Loss: 0.4884, Train: 96.50%, Valid: 55.25% Test: 50.92%\n",
      "Epoch: 17, Loss: 0.4320, Train: 97.50%, Valid: 55.50% Test: 51.50%\n",
      "Epoch: 18, Loss: 0.3779, Train: 98.25%, Valid: 57.25% Test: 52.21%\n",
      "Epoch: 19, Loss: 0.3271, Train: 98.00%, Valid: 58.00% Test: 53.77%\n",
      "Epoch: 20, Loss: 0.2803, Train: 98.25%, Valid: 58.00% Test: 54.95%\n",
      "Epoch: 21, Loss: 0.2382, Train: 98.50%, Valid: 57.25% Test: 55.38%\n",
      "Epoch: 22, Loss: 0.2010, Train: 99.00%, Valid: 57.50% Test: 55.43%\n",
      "Epoch: 23, Loss: 0.1688, Train: 99.00%, Valid: 58.50% Test: 55.75%\n",
      "Epoch: 24, Loss: 0.1414, Train: 99.00%, Valid: 58.25% Test: 55.84%\n",
      "Epoch: 25, Loss: 0.1185, Train: 99.00%, Valid: 59.25% Test: 55.87%\n",
      "cpu\n",
      "Epoch: 01, Loss: 1.1088, Train: 29.50%, Valid: 22.75% Test: 28.84%\n",
      "Epoch: 02, Loss: 1.0890, Train: 93.00%, Valid: 54.50% Test: 51.18%\n",
      "Epoch: 03, Loss: 1.0678, Train: 89.75%, Valid: 55.25% Test: 49.31%\n",
      "Epoch: 04, Loss: 1.0431, Train: 86.50%, Valid: 53.75% Test: 46.75%\n",
      "Epoch: 05, Loss: 1.0146, Train: 86.50%, Valid: 51.25% Test: 46.29%\n",
      "Epoch: 06, Loss: 0.9819, Train: 87.00%, Valid: 51.25% Test: 45.63%\n",
      "Epoch: 07, Loss: 0.9449, Train: 86.75%, Valid: 50.75% Test: 45.86%\n",
      "Epoch: 08, Loss: 0.9039, Train: 87.50%, Valid: 50.50% Test: 45.86%\n",
      "Epoch: 09, Loss: 0.8589, Train: 88.75%, Valid: 51.25% Test: 46.18%\n",
      "Epoch: 10, Loss: 0.8105, Train: 90.25%, Valid: 51.75% Test: 46.78%\n",
      "Epoch: 11, Loss: 0.7590, Train: 90.75%, Valid: 52.75% Test: 47.41%\n",
      "Epoch: 12, Loss: 0.7051, Train: 92.75%, Valid: 53.25% Test: 47.96%\n",
      "Epoch: 13, Loss: 0.6493, Train: 93.75%, Valid: 54.50% Test: 48.82%\n",
      "Epoch: 14, Loss: 0.5923, Train: 95.25%, Valid: 55.00% Test: 49.83%\n",
      "Epoch: 15, Loss: 0.5349, Train: 95.75%, Valid: 55.00% Test: 50.60%\n",
      "Epoch: 16, Loss: 0.4780, Train: 97.00%, Valid: 56.50% Test: 51.32%\n",
      "Epoch: 17, Loss: 0.4226, Train: 97.25%, Valid: 57.25% Test: 51.78%\n",
      "Epoch: 18, Loss: 0.3697, Train: 97.50%, Valid: 58.25% Test: 53.19%\n",
      "Epoch: 19, Loss: 0.3202, Train: 98.00%, Valid: 59.25% Test: 54.05%\n",
      "Epoch: 20, Loss: 0.2749, Train: 98.00%, Valid: 60.00% Test: 55.03%\n",
      "Epoch: 21, Loss: 0.2341, Train: 98.50%, Valid: 60.25% Test: 55.29%\n",
      "Epoch: 22, Loss: 0.1983, Train: 98.75%, Valid: 60.25% Test: 55.29%\n",
      "Epoch: 23, Loss: 0.1672, Train: 99.00%, Valid: 60.50% Test: 55.52%\n",
      "Epoch: 24, Loss: 0.1408, Train: 99.00%, Valid: 60.00% Test: 55.89%\n",
      "Epoch: 25, Loss: 0.1186, Train: 99.00%, Valid: 59.25% Test: 55.84%\n",
      "cpu\n",
      "Epoch: 01, Loss: 1.0931, Train: 41.75%, Valid: 40.25% Test: 36.66%\n",
      "Epoch: 02, Loss: 1.0749, Train: 53.00%, Valid: 41.00% Test: 37.03%\n",
      "Epoch: 03, Loss: 1.0551, Train: 63.25%, Valid: 43.25% Test: 37.98%\n",
      "Epoch: 04, Loss: 1.0319, Train: 66.50%, Valid: 44.75% Test: 39.16%\n",
      "Epoch: 05, Loss: 1.0047, Train: 70.25%, Valid: 46.00% Test: 40.43%\n",
      "Epoch: 06, Loss: 0.9733, Train: 72.50%, Valid: 47.25% Test: 41.17%\n",
      "Epoch: 07, Loss: 0.9377, Train: 74.25%, Valid: 47.25% Test: 41.69%\n",
      "Epoch: 08, Loss: 0.8979, Train: 77.50%, Valid: 47.50% Test: 42.50%\n",
      "Epoch: 09, Loss: 0.8541, Train: 80.75%, Valid: 48.75% Test: 43.24%\n",
      "Epoch: 10, Loss: 0.8065, Train: 85.75%, Valid: 49.00% Test: 44.42%\n",
      "Epoch: 11, Loss: 0.7555, Train: 88.50%, Valid: 50.00% Test: 45.77%\n",
      "Epoch: 12, Loss: 0.7017, Train: 92.00%, Valid: 51.25% Test: 46.72%\n",
      "Epoch: 13, Loss: 0.6456, Train: 93.00%, Valid: 52.75% Test: 47.99%\n",
      "Epoch: 14, Loss: 0.5881, Train: 94.50%, Valid: 54.75% Test: 49.25%\n",
      "Epoch: 15, Loss: 0.5302, Train: 96.00%, Valid: 55.75% Test: 50.52%\n",
      "Epoch: 16, Loss: 0.4728, Train: 96.75%, Valid: 57.25% Test: 51.24%\n",
      "Epoch: 17, Loss: 0.4170, Train: 97.00%, Valid: 58.50% Test: 52.50%\n",
      "Epoch: 18, Loss: 0.3638, Train: 97.75%, Valid: 59.25% Test: 53.22%\n",
      "Epoch: 19, Loss: 0.3141, Train: 98.00%, Valid: 59.00% Test: 53.88%\n",
      "Epoch: 20, Loss: 0.2687, Train: 98.25%, Valid: 58.75% Test: 54.80%\n",
      "Epoch: 21, Loss: 0.2280, Train: 98.50%, Valid: 59.00% Test: 55.41%\n",
      "Epoch: 22, Loss: 0.1924, Train: 99.00%, Valid: 59.00% Test: 55.66%\n",
      "Epoch: 23, Loss: 0.1616, Train: 99.00%, Valid: 59.25% Test: 55.41%\n",
      "Epoch: 24, Loss: 0.1356, Train: 99.00%, Valid: 58.25% Test: 55.61%\n",
      "Epoch: 25, Loss: 0.1138, Train: 99.00%, Valid: 58.00% Test: 55.69%\n",
      "cpu\n",
      "Epoch: 01, Loss: 1.0893, Train: 39.75%, Valid: 39.50% Test: 36.43%\n",
      "Epoch: 02, Loss: 1.0709, Train: 42.75%, Valid: 40.25% Test: 36.52%\n",
      "Epoch: 03, Loss: 1.0507, Train: 51.75%, Valid: 40.50% Test: 37.35%\n",
      "Epoch: 04, Loss: 1.0269, Train: 61.50%, Valid: 42.25% Test: 38.21%\n",
      "Epoch: 05, Loss: 0.9992, Train: 65.75%, Valid: 44.50% Test: 39.74%\n",
      "Epoch: 06, Loss: 0.9674, Train: 69.00%, Valid: 45.75% Test: 40.43%\n",
      "Epoch: 07, Loss: 0.9313, Train: 71.25%, Valid: 47.25% Test: 41.81%\n",
      "Epoch: 08, Loss: 0.8912, Train: 74.75%, Valid: 48.00% Test: 42.73%\n",
      "Epoch: 09, Loss: 0.8471, Train: 76.75%, Valid: 48.00% Test: 43.73%\n",
      "Epoch: 10, Loss: 0.7994, Train: 81.50%, Valid: 50.25% Test: 45.08%\n",
      "Epoch: 11, Loss: 0.7486, Train: 85.50%, Valid: 52.00% Test: 46.18%\n",
      "Epoch: 12, Loss: 0.6951, Train: 89.00%, Valid: 51.75% Test: 46.75%\n",
      "Epoch: 13, Loss: 0.6398, Train: 90.50%, Valid: 53.75% Test: 48.02%\n",
      "Epoch: 14, Loss: 0.5835, Train: 94.00%, Valid: 54.50% Test: 49.60%\n",
      "Epoch: 15, Loss: 0.5272, Train: 94.75%, Valid: 55.50% Test: 50.75%\n",
      "Epoch: 16, Loss: 0.4717, Train: 96.00%, Valid: 55.75% Test: 51.55%\n",
      "Epoch: 17, Loss: 0.4180, Train: 97.00%, Valid: 56.25% Test: 52.47%\n",
      "Epoch: 18, Loss: 0.3668, Train: 97.00%, Valid: 58.00% Test: 53.51%\n",
      "Epoch: 19, Loss: 0.3190, Train: 97.75%, Valid: 59.00% Test: 54.31%\n",
      "Epoch: 20, Loss: 0.2751, Train: 98.25%, Valid: 59.50% Test: 54.95%\n",
      "Epoch: 21, Loss: 0.2354, Train: 98.75%, Valid: 59.75% Test: 55.49%\n",
      "Epoch: 22, Loss: 0.2003, Train: 99.00%, Valid: 59.75% Test: 55.84%\n",
      "Epoch: 23, Loss: 0.1697, Train: 99.00%, Valid: 59.50% Test: 55.72%\n",
      "Epoch: 24, Loss: 0.1435, Train: 99.00%, Valid: 59.25% Test: 56.04%\n",
      "Epoch: 25, Loss: 0.1214, Train: 99.00%, Valid: 58.75% Test: 56.01%\n",
      "cpu\n",
      "Epoch: 01, Loss: 1.0899, Train: 40.75%, Valid: 39.50% Test: 36.49%\n",
      "Epoch: 02, Loss: 1.0708, Train: 46.50%, Valid: 40.50% Test: 36.63%\n",
      "Epoch: 03, Loss: 1.0493, Train: 54.75%, Valid: 40.75% Test: 37.44%\n",
      "Epoch: 04, Loss: 1.0241, Train: 64.00%, Valid: 42.00% Test: 38.27%\n",
      "Epoch: 05, Loss: 0.9946, Train: 68.75%, Valid: 43.75% Test: 39.39%\n",
      "Epoch: 06, Loss: 0.9608, Train: 72.25%, Valid: 46.50% Test: 40.40%\n",
      "Epoch: 07, Loss: 0.9226, Train: 75.75%, Valid: 46.75% Test: 41.55%\n",
      "Epoch: 08, Loss: 0.8800, Train: 79.75%, Valid: 47.50% Test: 42.87%\n",
      "Epoch: 09, Loss: 0.8333, Train: 86.25%, Valid: 48.25% Test: 44.22%\n",
      "Epoch: 10, Loss: 0.7827, Train: 90.75%, Valid: 50.00% Test: 45.43%\n",
      "Epoch: 11, Loss: 0.7287, Train: 92.25%, Valid: 51.25% Test: 46.58%\n",
      "Epoch: 12, Loss: 0.6719, Train: 94.00%, Valid: 52.25% Test: 48.02%\n",
      "Epoch: 13, Loss: 0.6132, Train: 95.25%, Valid: 54.50% Test: 49.31%\n",
      "Epoch: 14, Loss: 0.5535, Train: 96.75%, Valid: 56.25% Test: 50.69%\n",
      "Epoch: 15, Loss: 0.4939, Train: 96.75%, Valid: 57.00% Test: 51.64%\n",
      "Epoch: 16, Loss: 0.4358, Train: 97.50%, Valid: 58.25% Test: 53.28%\n",
      "Epoch: 17, Loss: 0.3802, Train: 98.00%, Valid: 58.25% Test: 54.08%\n",
      "Epoch: 18, Loss: 0.3282, Train: 98.25%, Valid: 59.25% Test: 54.80%\n",
      "Epoch: 19, Loss: 0.2806, Train: 98.25%, Valid: 57.50% Test: 54.97%\n",
      "Epoch: 20, Loss: 0.2381, Train: 98.50%, Valid: 58.50% Test: 55.32%\n",
      "Epoch: 21, Loss: 0.2007, Train: 99.00%, Valid: 58.75% Test: 55.26%\n",
      "Epoch: 22, Loss: 0.1685, Train: 99.00%, Valid: 58.75% Test: 55.43%\n",
      "Epoch: 23, Loss: 0.1412, Train: 99.00%, Valid: 58.25% Test: 55.35%\n",
      "Epoch: 24, Loss: 0.1182, Train: 99.00%, Valid: 58.25% Test: 55.29%\n",
      "Epoch: 25, Loss: 0.0991, Train: 99.00%, Valid: 58.50% Test: 55.18%\n"
     ]
    }
   ],
   "source": [
    "test_acc_list = []\n",
    "for seed in range (5):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Model Parameters\n",
    "    emb_dim = 128\n",
    "    hidden_dim = 128\n",
    "    output_dim = num_classes\n",
    "    num_layers = 3\n",
    "\n",
    "    model = SplitGCN(data.metadata(), emb_dim, hidden_dim, output_dim, num_layers, device)\n",
    "\n",
    "    model, data = model.to(device), data.to(device)\n",
    "\n",
    "    print(next(model.parameters()).device)\n",
    "\n",
    "    # Reset model parameters\n",
    "    # model.reset_parameters()\n",
    "\n",
    "    # Define hyperparameters\n",
    "    num_epochs = 25\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    loss_fn = F.cross_entropy\n",
    "\n",
    "    best_model = None\n",
    "    best_test_acc = 0\n",
    "\n",
    "    for epoch in range(1, 1 + num_epochs):\n",
    "        # train model\n",
    "        loss = train(model, data, optimizer, loss_fn)\n",
    "        \n",
    "        # evaluate model \n",
    "        result = test(model, data)\n",
    "        \n",
    "        train_acc, valid_acc, test_acc = result\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        print(f'Epoch: {epoch:02d}, '\n",
    "                f'Loss: {loss:.4f}, '\n",
    "                f'Train: {100 * train_acc:.2f}%, '\n",
    "                f'Valid: {100 * valid_acc:.2f}% '\n",
    "                f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "    test_acc_list.append(best_test_acc)\n",
    "    with open(\"scores_revised.txt\", \"a\") as f:\n",
    "        f.write( 'No KG Embeddings best test acc: ' + str(best_test_acc) + '\\n')\n",
    "\n",
    "avg_acc = sum(test_acc_list)/5\n",
    "with open(\"scores_revised.txt\", \"a\") as f:\n",
    "    f.write('Average accuracy: ' + str(avg_acc) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Evaluate with look-up embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relation embeddings: 2\n"
     ]
    }
   ],
   "source": [
    "num_clf_nodes = data[data.node_types[0]].x.shape[0]\n",
    "embs = [None, None]\n",
    "embs[0] = torch.Tensor([0,1])\n",
    "embs[1] = torch.Tensor([1,0])\n",
    "rel_embs_list = []\n",
    "for i in range (len(embs)):\n",
    "    rel_embs_list.append(torch.tensor(np.tile(embs[i], (num_clf_nodes, 1))))\n",
    "print('Number of relation embeddings:', len(rel_embs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch: 01, Loss: 1.0966, Train: 39.75%, Valid: 39.50% Test: 36.43%\n",
      "Epoch: 02, Loss: 1.0775, Train: 39.75%, Valid: 39.50% Test: 36.43%\n",
      "Epoch: 03, Loss: 1.0572, Train: 42.00%, Valid: 40.00% Test: 36.49%\n",
      "Epoch: 04, Loss: 1.0337, Train: 51.50%, Valid: 40.50% Test: 36.75%\n",
      "Epoch: 05, Loss: 1.0065, Train: 62.50%, Valid: 42.50% Test: 37.98%\n",
      "Epoch: 06, Loss: 0.9755, Train: 69.25%, Valid: 43.75% Test: 39.62%\n",
      "Epoch: 07, Loss: 0.9403, Train: 72.25%, Valid: 45.75% Test: 41.69%\n",
      "Epoch: 08, Loss: 0.9012, Train: 73.50%, Valid: 46.75% Test: 43.42%\n",
      "Epoch: 09, Loss: 0.8580, Train: 76.50%, Valid: 47.75% Test: 44.45%\n",
      "Epoch: 10, Loss: 0.8110, Train: 78.75%, Valid: 50.50% Test: 45.54%\n",
      "Epoch: 11, Loss: 0.7606, Train: 82.50%, Valid: 51.75% Test: 45.86%\n",
      "Epoch: 12, Loss: 0.7072, Train: 86.00%, Valid: 53.25% Test: 46.84%\n",
      "Epoch: 13, Loss: 0.6516, Train: 89.25%, Valid: 53.75% Test: 47.67%\n",
      "Epoch: 14, Loss: 0.5947, Train: 91.50%, Valid: 55.50% Test: 48.79%\n",
      "Epoch: 15, Loss: 0.5373, Train: 93.75%, Valid: 54.75% Test: 50.12%\n",
      "Epoch: 16, Loss: 0.4805, Train: 94.75%, Valid: 55.75% Test: 51.06%\n",
      "Epoch: 17, Loss: 0.4252, Train: 96.50%, Valid: 56.50% Test: 51.44%\n",
      "Epoch: 18, Loss: 0.3724, Train: 97.25%, Valid: 58.25% Test: 52.56%\n",
      "Epoch: 19, Loss: 0.3228, Train: 97.75%, Valid: 60.00% Test: 53.22%\n",
      "Epoch: 20, Loss: 0.2771, Train: 98.00%, Valid: 58.75% Test: 54.08%\n",
      "Epoch: 21, Loss: 0.2357, Train: 98.50%, Valid: 58.75% Test: 55.06%\n",
      "Epoch: 22, Loss: 0.1991, Train: 98.75%, Valid: 59.00% Test: 54.95%\n",
      "Epoch: 23, Loss: 0.1673, Train: 99.00%, Valid: 59.50% Test: 55.29%\n",
      "Epoch: 24, Loss: 0.1403, Train: 99.00%, Valid: 59.50% Test: 55.43%\n",
      "Epoch: 25, Loss: 0.1176, Train: 99.00%, Valid: 59.50% Test: 55.35%\n",
      "Epoch: 26, Loss: 0.0990, Train: 99.00%, Valid: 59.75% Test: 55.49%\n",
      "Epoch: 27, Loss: 0.0837, Train: 99.00%, Valid: 59.25% Test: 55.35%\n",
      "Epoch: 28, Loss: 0.0713, Train: 99.00%, Valid: 59.00% Test: 55.35%\n",
      "Epoch: 29, Loss: 0.0612, Train: 99.00%, Valid: 58.75% Test: 55.52%\n",
      "Epoch: 30, Loss: 0.0531, Train: 99.00%, Valid: 58.50% Test: 55.43%\n",
      "cpu\n",
      "Epoch: 01, Loss: 1.0994, Train: 45.50%, Valid: 41.25% Test: 36.98%\n",
      "Epoch: 02, Loss: 1.0806, Train: 57.75%, Valid: 42.75% Test: 38.13%\n",
      "Epoch: 03, Loss: 1.0607, Train: 64.50%, Valid: 45.75% Test: 39.39%\n",
      "Epoch: 04, Loss: 1.0379, Train: 67.75%, Valid: 45.75% Test: 40.68%\n",
      "Epoch: 05, Loss: 1.0116, Train: 70.75%, Valid: 45.75% Test: 41.20%\n",
      "Epoch: 06, Loss: 0.9815, Train: 71.75%, Valid: 46.00% Test: 41.81%\n",
      "Epoch: 07, Loss: 0.9474, Train: 73.00%, Valid: 46.25% Test: 42.09%\n",
      "Epoch: 08, Loss: 0.9092, Train: 74.50%, Valid: 47.00% Test: 42.50%\n",
      "Epoch: 09, Loss: 0.8671, Train: 76.00%, Valid: 47.50% Test: 43.10%\n",
      "Epoch: 10, Loss: 0.8210, Train: 82.25%, Valid: 48.25% Test: 43.96%\n",
      "Epoch: 11, Loss: 0.7713, Train: 87.00%, Valid: 49.00% Test: 44.71%\n",
      "Epoch: 12, Loss: 0.7182, Train: 90.50%, Valid: 51.50% Test: 45.57%\n",
      "Epoch: 13, Loss: 0.6624, Train: 93.25%, Valid: 52.50% Test: 47.12%\n",
      "Epoch: 14, Loss: 0.6045, Train: 94.25%, Valid: 52.75% Test: 48.62%\n",
      "Epoch: 15, Loss: 0.5457, Train: 95.50%, Valid: 54.75% Test: 49.88%\n",
      "Epoch: 16, Loss: 0.4869, Train: 96.25%, Valid: 56.00% Test: 51.01%\n",
      "Epoch: 17, Loss: 0.4295, Train: 97.00%, Valid: 57.50% Test: 52.10%\n",
      "Epoch: 18, Loss: 0.3745, Train: 97.75%, Valid: 57.25% Test: 53.39%\n",
      "Epoch: 19, Loss: 0.3231, Train: 98.25%, Valid: 57.25% Test: 54.17%\n",
      "Epoch: 20, Loss: 0.2760, Train: 99.00%, Valid: 57.50% Test: 54.83%\n",
      "Epoch: 21, Loss: 0.2339, Train: 99.00%, Valid: 57.50% Test: 55.38%\n",
      "Epoch: 22, Loss: 0.1971, Train: 99.00%, Valid: 57.50% Test: 55.66%\n",
      "Epoch: 23, Loss: 0.1654, Train: 99.00%, Valid: 57.75% Test: 55.81%\n",
      "Epoch: 24, Loss: 0.1386, Train: 99.00%, Valid: 57.75% Test: 55.84%\n",
      "Epoch: 25, Loss: 0.1161, Train: 99.00%, Valid: 57.25% Test: 55.61%\n",
      "Epoch: 26, Loss: 0.0976, Train: 99.00%, Valid: 57.50% Test: 55.64%\n",
      "Epoch: 27, Loss: 0.0822, Train: 99.00%, Valid: 57.75% Test: 55.81%\n",
      "Epoch: 28, Loss: 0.0697, Train: 99.00%, Valid: 57.75% Test: 56.04%\n",
      "Epoch: 29, Loss: 0.0595, Train: 99.00%, Valid: 58.25% Test: 55.98%\n",
      "Epoch: 30, Loss: 0.0511, Train: 99.00%, Valid: 58.50% Test: 55.98%\n",
      "cpu\n",
      "Epoch: 01, Loss: 1.0987, Train: 40.00%, Valid: 39.50% Test: 36.52%\n",
      "Epoch: 02, Loss: 1.0799, Train: 43.25%, Valid: 39.75% Test: 36.57%\n",
      "Epoch: 03, Loss: 1.0601, Train: 51.50%, Valid: 41.00% Test: 37.09%\n",
      "Epoch: 04, Loss: 1.0373, Train: 59.25%, Valid: 41.75% Test: 37.87%\n",
      "Epoch: 05, Loss: 1.0109, Train: 64.25%, Valid: 43.00% Test: 38.79%\n",
      "Epoch: 06, Loss: 0.9806, Train: 68.75%, Valid: 44.75% Test: 39.82%\n",
      "Epoch: 07, Loss: 0.9464, Train: 71.00%, Valid: 46.00% Test: 40.77%\n",
      "Epoch: 08, Loss: 0.9080, Train: 75.00%, Valid: 47.25% Test: 41.98%\n",
      "Epoch: 09, Loss: 0.8656, Train: 78.25%, Valid: 48.00% Test: 42.98%\n",
      "Epoch: 10, Loss: 0.8192, Train: 83.00%, Valid: 48.75% Test: 44.51%\n",
      "Epoch: 11, Loss: 0.7689, Train: 87.75%, Valid: 50.25% Test: 45.69%\n",
      "Epoch: 12, Loss: 0.7152, Train: 91.25%, Valid: 52.00% Test: 46.92%\n",
      "Epoch: 13, Loss: 0.6586, Train: 94.00%, Valid: 53.50% Test: 48.45%\n",
      "Epoch: 14, Loss: 0.6002, Train: 95.25%, Valid: 54.25% Test: 49.71%\n",
      "Epoch: 15, Loss: 0.5409, Train: 96.25%, Valid: 56.00% Test: 50.92%\n",
      "Epoch: 16, Loss: 0.4821, Train: 97.00%, Valid: 58.50% Test: 52.19%\n",
      "Epoch: 17, Loss: 0.4248, Train: 98.25%, Valid: 58.75% Test: 53.25%\n",
      "Epoch: 18, Loss: 0.3704, Train: 98.50%, Valid: 59.25% Test: 54.43%\n",
      "Epoch: 19, Loss: 0.3199, Train: 98.50%, Valid: 59.75% Test: 55.52%\n",
      "Epoch: 20, Loss: 0.2740, Train: 98.50%, Valid: 60.00% Test: 55.32%\n",
      "Epoch: 21, Loss: 0.2330, Train: 98.75%, Valid: 60.00% Test: 55.52%\n",
      "Epoch: 22, Loss: 0.1972, Train: 99.00%, Valid: 60.25% Test: 55.75%\n",
      "Epoch: 23, Loss: 0.1663, Train: 99.00%, Valid: 59.75% Test: 55.87%\n",
      "Epoch: 24, Loss: 0.1400, Train: 99.00%, Valid: 59.25% Test: 55.72%\n",
      "Epoch: 25, Loss: 0.1177, Train: 99.00%, Valid: 58.75% Test: 55.69%\n",
      "Epoch: 26, Loss: 0.0991, Train: 99.00%, Valid: 58.50% Test: 55.69%\n",
      "Epoch: 27, Loss: 0.0836, Train: 99.00%, Valid: 58.25% Test: 55.64%\n",
      "Epoch: 28, Loss: 0.0709, Train: 99.00%, Valid: 58.75% Test: 55.61%\n",
      "Epoch: 29, Loss: 0.0605, Train: 99.00%, Valid: 58.25% Test: 55.49%\n",
      "Epoch: 30, Loss: 0.0522, Train: 99.00%, Valid: 57.75% Test: 55.49%\n",
      "cpu\n",
      "Epoch: 01, Loss: 1.0955, Train: 39.75%, Valid: 39.50% Test: 36.43%\n",
      "Epoch: 02, Loss: 1.0761, Train: 41.25%, Valid: 39.50% Test: 36.49%\n",
      "Epoch: 03, Loss: 1.0555, Train: 46.00%, Valid: 40.25% Test: 36.77%\n",
      "Epoch: 04, Loss: 1.0320, Train: 56.50%, Valid: 41.50% Test: 37.03%\n",
      "Epoch: 05, Loss: 1.0050, Train: 63.50%, Valid: 43.00% Test: 38.13%\n",
      "Epoch: 06, Loss: 0.9742, Train: 67.50%, Valid: 43.50% Test: 39.10%\n",
      "Epoch: 07, Loss: 0.9394, Train: 71.25%, Valid: 44.75% Test: 40.20%\n",
      "Epoch: 08, Loss: 0.9005, Train: 77.00%, Valid: 46.00% Test: 41.32%\n",
      "Epoch: 09, Loss: 0.8574, Train: 82.75%, Valid: 47.75% Test: 42.78%\n",
      "Epoch: 10, Loss: 0.8102, Train: 87.50%, Valid: 48.50% Test: 44.02%\n",
      "Epoch: 11, Loss: 0.7590, Train: 91.50%, Valid: 50.25% Test: 45.89%\n",
      "Epoch: 12, Loss: 0.7044, Train: 94.00%, Valid: 51.25% Test: 47.47%\n",
      "Epoch: 13, Loss: 0.6471, Train: 95.75%, Valid: 53.00% Test: 49.48%\n",
      "Epoch: 14, Loss: 0.5881, Train: 96.50%, Valid: 55.25% Test: 50.92%\n",
      "Epoch: 15, Loss: 0.5286, Train: 97.00%, Valid: 57.50% Test: 52.01%\n",
      "Epoch: 16, Loss: 0.4699, Train: 97.75%, Valid: 58.75% Test: 53.22%\n",
      "Epoch: 17, Loss: 0.4133, Train: 98.00%, Valid: 58.50% Test: 55.03%\n",
      "Epoch: 18, Loss: 0.3599, Train: 98.00%, Valid: 58.00% Test: 55.26%\n",
      "Epoch: 19, Loss: 0.3108, Train: 98.00%, Valid: 57.75% Test: 55.55%\n",
      "Epoch: 20, Loss: 0.2664, Train: 98.25%, Valid: 58.25% Test: 55.52%\n",
      "Epoch: 21, Loss: 0.2270, Train: 98.25%, Valid: 58.00% Test: 55.61%\n",
      "Epoch: 22, Loss: 0.1925, Train: 99.00%, Valid: 58.25% Test: 55.64%\n",
      "Epoch: 23, Loss: 0.1627, Train: 99.00%, Valid: 57.75% Test: 55.87%\n",
      "Epoch: 24, Loss: 0.1371, Train: 99.00%, Valid: 57.50% Test: 55.84%\n",
      "Epoch: 25, Loss: 0.1154, Train: 99.00%, Valid: 58.00% Test: 55.78%\n",
      "Epoch: 26, Loss: 0.0971, Train: 99.00%, Valid: 58.00% Test: 55.61%\n",
      "Epoch: 27, Loss: 0.0820, Train: 99.00%, Valid: 58.00% Test: 55.61%\n",
      "Epoch: 28, Loss: 0.0695, Train: 99.00%, Valid: 58.50% Test: 55.52%\n",
      "Epoch: 29, Loss: 0.0593, Train: 99.00%, Valid: 58.50% Test: 55.38%\n",
      "Epoch: 30, Loss: 0.0511, Train: 99.00%, Valid: 58.75% Test: 55.58%\n",
      "cpu\n",
      "Epoch: 01, Loss: 1.0980, Train: 49.75%, Valid: 42.25% Test: 37.49%\n",
      "Epoch: 02, Loss: 1.0786, Train: 59.50%, Valid: 42.25% Test: 38.13%\n",
      "Epoch: 03, Loss: 1.0579, Train: 64.75%, Valid: 43.00% Test: 38.90%\n",
      "Epoch: 04, Loss: 1.0343, Train: 68.25%, Valid: 44.00% Test: 39.94%\n",
      "Epoch: 05, Loss: 1.0070, Train: 69.75%, Valid: 45.50% Test: 40.54%\n",
      "Epoch: 06, Loss: 0.9759, Train: 71.25%, Valid: 46.25% Test: 41.09%\n",
      "Epoch: 07, Loss: 0.9407, Train: 72.75%, Valid: 47.25% Test: 41.60%\n",
      "Epoch: 08, Loss: 0.9015, Train: 74.25%, Valid: 48.25% Test: 42.41%\n",
      "Epoch: 09, Loss: 0.8583, Train: 76.25%, Valid: 48.25% Test: 43.04%\n",
      "Epoch: 10, Loss: 0.8111, Train: 80.50%, Valid: 47.75% Test: 43.79%\n",
      "Epoch: 11, Loss: 0.7601, Train: 86.50%, Valid: 48.75% Test: 44.80%\n",
      "Epoch: 12, Loss: 0.7058, Train: 90.00%, Valid: 49.50% Test: 45.97%\n",
      "Epoch: 13, Loss: 0.6489, Train: 92.50%, Valid: 51.75% Test: 47.12%\n",
      "Epoch: 14, Loss: 0.5903, Train: 94.50%, Valid: 53.25% Test: 48.27%\n",
      "Epoch: 15, Loss: 0.5310, Train: 96.00%, Valid: 55.75% Test: 49.91%\n",
      "Epoch: 16, Loss: 0.4722, Train: 96.75%, Valid: 57.00% Test: 51.06%\n",
      "Epoch: 17, Loss: 0.4151, Train: 97.25%, Valid: 57.75% Test: 51.93%\n",
      "Epoch: 18, Loss: 0.3609, Train: 98.25%, Valid: 58.00% Test: 53.34%\n",
      "Epoch: 19, Loss: 0.3105, Train: 98.00%, Valid: 58.50% Test: 54.63%\n",
      "Epoch: 20, Loss: 0.2648, Train: 98.25%, Valid: 58.00% Test: 55.12%\n",
      "Epoch: 21, Loss: 0.2240, Train: 98.50%, Valid: 58.25% Test: 55.49%\n",
      "Epoch: 22, Loss: 0.1885, Train: 99.00%, Valid: 57.50% Test: 55.46%\n",
      "Epoch: 23, Loss: 0.1581, Train: 99.00%, Valid: 58.50% Test: 55.75%\n",
      "Epoch: 24, Loss: 0.1324, Train: 99.00%, Valid: 58.25% Test: 55.61%\n",
      "Epoch: 25, Loss: 0.1110, Train: 99.00%, Valid: 58.00% Test: 55.72%\n",
      "Epoch: 26, Loss: 0.0933, Train: 99.00%, Valid: 59.00% Test: 55.81%\n",
      "Epoch: 27, Loss: 0.0787, Train: 99.00%, Valid: 59.00% Test: 55.84%\n",
      "Epoch: 28, Loss: 0.0668, Train: 99.00%, Valid: 59.00% Test: 55.75%\n",
      "Epoch: 29, Loss: 0.0571, Train: 99.00%, Valid: 58.25% Test: 55.69%\n",
      "Epoch: 30, Loss: 0.0492, Train: 99.00%, Valid: 58.50% Test: 55.69%\n"
     ]
    }
   ],
   "source": [
    "test_acc_list = []\n",
    "for seed in range (5):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Model Parameters\n",
    "    emb_dim = 128\n",
    "    hidden_dim = 128\n",
    "    output_dim = num_classes\n",
    "    num_layers = 3\n",
    "\n",
    "    model = SplitGCN(data.metadata(), emb_dim, hidden_dim, output_dim, num_layers, device, rel_embs_list)\n",
    "\n",
    "    model, data = model.to(device), data.to(device)\n",
    "\n",
    "    print(next(model.parameters()).device)\n",
    "\n",
    "    # Reset model parameters\n",
    "    # model.reset_parameters()\n",
    "\n",
    "    # Define hyperparameters\n",
    "    num_epochs = 30\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    loss_fn = F.cross_entropy\n",
    "\n",
    "    best_model = None\n",
    "    best_test_acc = 0\n",
    "\n",
    "    for epoch in range(1, 1 + num_epochs):\n",
    "        # train model\n",
    "        loss = train(model, data, optimizer, loss_fn, rel_embs_list)\n",
    "        \n",
    "        # evaluate model \n",
    "        result = test(model, data, rel_embs_list)\n",
    "        \n",
    "        train_acc, valid_acc, test_acc = result\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        print(f'Epoch: {epoch:02d}, '\n",
    "                f'Loss: {loss:.4f}, '\n",
    "                f'Train: {100 * train_acc:.2f}%, '\n",
    "                f'Valid: {100 * valid_acc:.2f}% '\n",
    "                f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "    test_acc_list.append(best_test_acc)\n",
    "    with open(\"scores_revised.txt\", \"a\") as f:\n",
    "        f.write( 'Look-up Embedding best test acc: ' + str(best_test_acc) + '\\n')\n",
    "\n",
    "avg_acc = sum(test_acc_list)/5\n",
    "with open(\"scores_revised.txt\", \"a\") as f:\n",
    "    f.write('Average accuracy: ' + str(avg_acc) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5030792b3492f6b12d94f1f48beca3d8e59ec05fd59d0aaaa48e684281ed297"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
