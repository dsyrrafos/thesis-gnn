{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Relations Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTroch Version 1.10.2\n",
      "GPU Available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print('PyTroch Version', torch.__version__)\n",
    "print('GPU Available:', torch.cuda.is_available())\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Key             | Value                    |\n",
      "|-----------------|--------------------------|\n",
      "| OS              | posix                    |\n",
      "| Platform        | Linux                    |\n",
      "| Release         | 5.13.0-30-generic        |\n",
      "| Time            | Sun Apr  3 22:30:15 2022 |\n",
      "| Python          | 3.9.7                    |\n",
      "| PyKEEN          | 1.7.0                    |\n",
      "| PyKEEN Hash     | UNHASHED                 |\n",
      "| PyKEEN Branch   |                          |\n",
      "| PyTorch         | 1.10.2                   |\n",
      "| CUDA Available? | false                    |\n",
      "| CUDA Version    | N/A                      |\n",
      "| cuDNN Version   | N/A                      |\n"
     ]
    }
   ],
   "source": [
    "import pykeen\n",
    "pykeen.env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: IMDB()\n",
      "Number of graphs: 1\n",
      "HeteroData(\n",
      "  \u001b[1mmovie\u001b[0m={\n",
      "    x=[4278, 3066],\n",
      "    y=[4278],\n",
      "    train_mask=[4278],\n",
      "    val_mask=[4278],\n",
      "    test_mask=[4278]\n",
      "  },\n",
      "  \u001b[1mdirector\u001b[0m={ x=[2081, 3066] },\n",
      "  \u001b[1mactor\u001b[0m={ x=[5257, 3066] },\n",
      "  \u001b[1m(movie, to, director)\u001b[0m={ edge_index=[2, 4278] },\n",
      "  \u001b[1m(movie, to, actor)\u001b[0m={ edge_index=[2, 12828] },\n",
      "  \u001b[1m(director, to, movie)\u001b[0m={ edge_index=[2, 4278] },\n",
      "  \u001b[1m(actor, to, movie)\u001b[0m={ edge_index=[2, 12828] }\n",
      ")\n",
      "Number of classes: 3\n",
      "Classes: tensor([0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import IMDB\n",
    "\n",
    "dataset = IMDB(root='./data/imdb')\n",
    "print('Dataset:', dataset)\n",
    "\n",
    "print('Number of graphs:', len(dataset))\n",
    "\n",
    "data = dataset[0]\n",
    "print(data)\n",
    "\n",
    "num_classes = len(data['movie'].y.unique())\n",
    "print('Number of classes:', num_classes)\n",
    "print('Classes:', data['movie'].y.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TransR embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating triples\n",
    "\n",
    "We want to create a (n,3)-tensor where each row will be a (node_id, relation_id, node_id) triple. We have 3 types of nodes: movie, director, actor and 4 types of realtions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranges of each node type\n",
      "Movie: 0 - 4277\n",
      "Director: 4278 - 6358\n",
      "Actor: 6359 11615\n"
     ]
    }
   ],
   "source": [
    "print('Ranges of each node type')\n",
    "print('Movie:',0,'-',data['movie'].x.size()[0]-1)\n",
    "print('Director:', data['movie'].x.size()[0], '-', data['movie'].x.size()[0]+data['director'].x.size()[0]-1)\n",
    "print('Actor:', data['movie'].x.size()[0]+data['director'].x.size()[0], data['movie'].x.size()[0]+data['director'].x.size()[0]+data['actor'].x.size()[0]-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reindex tails in `movie_to_director` and `movie_to_actor` relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4278 2081\n",
      "torch.Size([2, 4278]) torch.Size([2, 12828])\n"
     ]
    }
   ],
   "source": [
    "movie_size = data['movie'].x.size()[0]\n",
    "director_size = data['director'].x.size()[0]\n",
    "print(movie_size, director_size)\n",
    "movie_size = data['movie'].x.size()[0]\n",
    "director_size = data['director'].x.size()[0]\n",
    "offset_director = torch.tensor([[0],[movie_size]])\n",
    "offset_director = offset_director.tile(1, data[('movie', 'to', 'director')].edge_index.size()[1])\n",
    "movie_to_director = data[('movie', 'to', 'director')].edge_index + offset_director\n",
    "offset_actor = torch.tensor([[0],[movie_size + director_size]])\n",
    "offset_actor = offset_actor.tile(1, data[('movie', 'to', 'actor')].edge_index.size()[1])\n",
    "movie_to_actor = data[('movie', 'to', 'actor')].edge_index + offset_actor\n",
    "print(movie_to_director.size(), movie_to_actor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `movie-to-actor`: 0\n",
    "- `movie-to-director`: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4278, 3]) torch.Size([12828, 3])\n"
     ]
    }
   ],
   "source": [
    "pad = torch.zeros(movie_to_actor.size()[1])\n",
    "movie_to_actor = torch.column_stack((movie_to_actor[0],pad,movie_to_actor[1]))\n",
    "pad = torch.ones(movie_to_director.size()[1])\n",
    "movie_to_director = torch.column_stack((movie_to_director[0],pad,movie_to_director[1]))\n",
    "print(movie_to_director.size(), movie_to_actor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17106, 3])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples = torch.concat((movie_to_director, movie_to_actor))\n",
    "triples.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 1.0000e+00, 5.0670e+03],\n",
       "        [1.0000e+00, 1.0000e+00, 4.9580e+03],\n",
       "        [2.0000e+00, 1.0000e+00, 6.0350e+03],\n",
       "        ...,\n",
       "        [4.2770e+03, 0.0000e+00, 6.4590e+03],\n",
       "        [4.2770e+03, 0.0000e+00, 7.4370e+03],\n",
       "        [4.2770e+03, 0.0000e+00, 7.7980e+03]])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_ids = [i for i in range (data.num_nodes)]\n",
    "relation_ids = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples.long().type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['TransR', 'TransH', 'RotatE', 'DistMult', 'ComplEx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.utils:No cuda devices were available. The model runs on CPU\n",
      "Training epochs on cpu: 100%|██████████| 50/50 [02:04<00:00,  2.49s/epoch, loss=0.000898, prev_loss=0.00107]\n",
      "INFO:pykeen.evaluation.evaluator:Currently automatic memory optimization only supports GPUs, but you're using a CPU. Therefore, the batch_size will be set to the default value.\n",
      "INFO:pykeen.evaluation.evaluator:No evaluation batch_size provided. Setting batch_size to '32'.\n",
      "Evaluating on cpu: 100%|██████████| 17.1k/17.1k [03:36<00:00, 79.2triple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 216.11s seconds\n"
     ]
    }
   ],
   "source": [
    "from pykeen.triples import CoreTriplesFactory\n",
    "from pykeen.pipeline import pipeline\n",
    "\n",
    "def train_kg_emb_model(model, triples, data, entity_ids, relation_ids):\n",
    "    # Load training data\n",
    "    emb_dim = 200\n",
    "    num_epochs = 50\n",
    "    training = CoreTriplesFactory(mapped_triples=triples.long(), num_entities=data.num_nodes, \n",
    "                                    num_relations=2, create_inverse_triples=False,\n",
    "                                    entity_ids=entity_ids, relation_ids=relation_ids)\n",
    "\n",
    "    result = pipeline(\n",
    "        training=training,\n",
    "        testing=training,\n",
    "        model=model,\n",
    "        random_seed=42,\n",
    "        model_kwargs={\"embedding_dim\":emb_dim},\n",
    "        training_kwargs={\"num_epochs\":num_epochs},\n",
    "        #stopper='early', # early stopping arguments. You need the validation set with this.\n",
    "        #stopper_kwargs=dict(frequency=3, patience=3, relative_delta=0.002),\n",
    "        #epochs=5,  # short epochs for testing - you should go higher\n",
    "    )\n",
    "\n",
    "    # Save mode to a directory. Yoy can load it afterwards\n",
    "    result.save_to_directory('Models_pykeen/imdb_' + model + '_ep_' + str(num_epochs) + '_dim_' + str(emb_dim))\n",
    "    return result\n",
    "\n",
    "for model in models:\n",
    "    result = train_kg_emb_model(model, triples, data, entity_ids, relation_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie', 'to', 'director'),\n",
       " ('movie', 'to', 'actor'),\n",
       " ('director', 'to', 'movie'),\n",
       " ('actor', 'to', 'movie')]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.metadata()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = SplitGCN(data.metadata(), 256, 256, num_classes, 2, device=device, transr=rel_embs_list)\\nprint(model)\\ndel model\\ngc.collect()\\n'"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "from torch_geometric.nn import Linear, HeteroConv, GCNConv, SAGEConv\n",
    "\n",
    "class SplitGCN(torch.nn.Module):\n",
    "    def __init__(self, metadata, emb_size, dense_size, out_size, num_layers, device='cpu', transr=None):\n",
    "        # TODO: Implement a function that initializes self.convs, \n",
    "        # self.bns, and self.softmax.\n",
    "        super(SplitGCN, self).__init__()\n",
    "\n",
    "        self.num_relations = int(len(metadata[1])/2)\n",
    "        self.device = device\n",
    "        self.edge_conv_dict = {}\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        '''\n",
    "        for edge_type in metadata[1]:\n",
    "            self.convs = nn.ModuleList()\n",
    "            for _ in range (num_layers):\n",
    "                self.conv = HeteroConv({\n",
    "                    edge_type: SAGEConv((-1,-1), emb_size)\n",
    "                })\n",
    "                self.convs.append(self.conv)\n",
    "            self.edge_conv_dict[edge_type] = self.convs\n",
    "        '''\n",
    "\n",
    "        for node_type in ['actor', 'director']:\n",
    "            self.conv = HeteroConv({\n",
    "                ('movie', 'to', node_type): SAGEConv((-1,-1), emb_size),\n",
    "                (node_type, 'to', 'movie'): SAGEConv((-1,-1), emb_size)\n",
    "            })\n",
    "            self.edge_conv_dict[node_type] = self.conv\n",
    "\n",
    "        if transr is not None:\n",
    "            self.transr = transr\n",
    "\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range (self.num_relations):\n",
    "            try:\n",
    "                linear = Linear(emb_size + self.transr[i].size()[1], dense_size)\n",
    "            except:\n",
    "                linear = Linear(emb_size, dense_size)\n",
    "            self.linears.append(linear)\n",
    "        \n",
    "        self.clflinear = Linear(self.num_relations*dense_size, out_size)\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, transr=None):\n",
    "        # TODO: Implement a function that takes the feature tensor x and\n",
    "        # edge_index tensor adj_t and returns the output tensor as\n",
    "        # shown in the figure.\n",
    "        '''\n",
    "        concatenated_embs = []\n",
    "        #print(len(self.convs))\n",
    "        for relation, edge_tensor in edge_index_dict.items():\n",
    "            # create one convolution subnetwork for each realtion\n",
    "            convs = self.edge_conv_dict[relation]\n",
    "\n",
    "            # isolate relations\n",
    "            single_edge_index_dict = {relation: edge_tensor.to(self.device)}\n",
    "            single_x_dict = {relation[0]: x_dict[relation[0]].to(self.device), \n",
    "                            relation[2]: x_dict[relation[2]].to(self.device)}\n",
    "\n",
    "            # forward propagate data into each relation-specific GNN\n",
    "            for i, conv in enumerate(convs):\n",
    "                print(i,conv)\n",
    "                print(single_x_dict.keys())\n",
    "                print(single_edge_index_dict.keys())\n",
    "                single_x_dict = conv(single_x_dict, single_edge_index_dict)\n",
    "            concatenated_embs.append(single_x_dict['movie'])\n",
    "        '''\n",
    "        \n",
    "        concatenated_embs = []\n",
    "        for node_type in ['actor', 'director']:\n",
    "            conv = self.edge_conv_dict[node_type]\n",
    "            \n",
    "            relation = ('movie', 'to', node_type)\n",
    "            reverse = (node_type, 'to', 'movie')\n",
    "\n",
    "            single_edge_index_dict = {relation: edge_index_dict[relation].to(self.device),\n",
    "                                        reverse: edge_index_dict[reverse].to(self.device)}\n",
    "            \n",
    "            single_x_dict = {relation[0]: x_dict[relation[0]].to(self.device), \n",
    "                            relation[2]: x_dict[relation[2]].to(self.device)}\n",
    "\n",
    "            single_x_dict = conv(single_x_dict, single_edge_index_dict)\n",
    "            concatenated_embs.append(single_x_dict['movie'])\n",
    "\n",
    "        linear_outputs = []\n",
    "        for i, (x, linear) in enumerate(zip(concatenated_embs, self.linears)):\n",
    "            #print(x.size())\n",
    "            #print(transr.size())\n",
    "            if transr:\n",
    "                x = torch.cat((x, transr[i]), 1)\n",
    "            x = linear(x)\n",
    "            #print('linear output', x.size())\n",
    "            linear_outputs.append(x)\n",
    "        \n",
    "        #print(len(linear_outputs))\n",
    "        out = torch.cat(linear_outputs, dim=-1)\n",
    "        #print('clf layer input', out.size())\n",
    "        out = self.clflinear(out)\n",
    "        #print('clf layer output', out.size())\n",
    "        return out\n",
    "\n",
    "'''\n",
    "model = SplitGCN(data.metadata(), 256, 256, num_classes, 2, device=device, transr=rel_embs_list)\n",
    "print(model)\n",
    "del model\n",
    "gc.collect()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, loss_fn, transr=None):\n",
    "    # TODO: Implement a function that trains the model by \n",
    "    # using the given optimizer and loss_fn.\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(data.x_dict, data.edge_index_dict, transr)\n",
    "    mask = data['movie'].train_mask\n",
    "    labels = data['movie'].y[mask]\n",
    "    loss = loss_fn(outputs[mask], labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function here\n",
    "@torch.no_grad()\n",
    "def test(model, data, transr=None, save_model_results=False):\n",
    "    # a function that tests the model by \n",
    "    # using the given split_idx and evaluator.\n",
    "    model.eval()\n",
    "\n",
    "    # The output of model on all data\n",
    "    out = model(data.x_dict, data.edge_index_dict, transr)\n",
    "    pred = out.argmax(dim=-1, keepdim=True)[:,0]\n",
    "    \n",
    "    accs = []\n",
    "    for split in ['train_mask', 'val_mask', 'test_mask']:\n",
    "        mask = data['movie'][split]\n",
    "        acc = (pred[mask] == data['movie'].y[mask]).sum() / mask.sum()\n",
    "        accs.append(float(acc))\n",
    "        #print(pred[mask].size(), data['movie'].y[mask].size())\n",
    "        #print(pred[mask].sum(), data['movie'].y[mask].sum())\n",
    "        #print(mask.sum(), (pred[mask] == data['movie'].y[mask]).sum())\n",
    "\n",
    "    if save_model_results:\n",
    "      print (\"Saving Model Predictions\")\n",
    "\n",
    "      data = {}\n",
    "      data['y_pred'] = pred.view(-1).cpu().detach().numpy()\n",
    "\n",
    "      df = pd.DataFrame(data=data)\n",
    "      # Save locally as csv\n",
    "      df.to_csv('imdb.csv', sep=',', index=False)\n",
    "\n",
    "\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Evaluate with KG Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def run_train_eval(data, kg_emb):\n",
    "\n",
    "    print('KG embedding model:', kg_emb)\n",
    "\n",
    "    transr_model = torch.load('Models_pykeen/' + kg_emb + '/trained_model.pkl')\n",
    "\n",
    "    transr_emb_entity = transr_model.entity_representations[0](indices=None).detach().numpy()\n",
    "    transr_emb_relation = transr_model.relation_representations[0](indices=None).detach().numpy()\n",
    "    num_clf_nodes = data[data.node_types[0]].x.shape[0]\n",
    "    embs = transr_emb_relation\n",
    "\n",
    "    rel_embs_list = []\n",
    "    for i in range (embs.shape[0]):\n",
    "        rel_embs_list.append(torch.tensor(np.tile(embs[i], (num_clf_nodes, 1))))\n",
    "    print('Number of relation embeddings:', len(rel_embs_list))\n",
    "\n",
    "    # Model Parameters\n",
    "    emb_dim = 128\n",
    "    hidden_dim = 128\n",
    "    output_dim = num_classes\n",
    "    num_layers = 3\n",
    "\n",
    "    model = SplitGCN(data.metadata(), emb_dim, hidden_dim, output_dim, num_layers, device, rel_embs_list)\n",
    "\n",
    "    model, data = model.to(device), data.to(device)\n",
    "\n",
    "    print(next(model.parameters()).device)\n",
    "\n",
    "    # Reset model parameters\n",
    "    # model.reset_parameters()\n",
    "\n",
    "    # Define hyperparameters\n",
    "    num_epochs = 30\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    loss_fn = F.cross_entropy\n",
    "\n",
    "    best_model = None\n",
    "    best_valid_acc = 0\n",
    "\n",
    "    for epoch in range(1, 1 + num_epochs):\n",
    "        # train model\n",
    "        loss = train(model, data, optimizer, loss_fn, rel_embs_list)\n",
    "        \n",
    "        # evaluate model \n",
    "        result = test(model, data, rel_embs_list)\n",
    "        \n",
    "        train_acc, valid_acc, test_acc = result\n",
    "        if valid_acc > best_valid_acc:\n",
    "            best_valid_acc = valid_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        print(f'Epoch: {epoch:02d}, '\n",
    "                f'Loss: {loss:.4f}, '\n",
    "                f'Train: {100 * train_acc:.2f}%, '\n",
    "                f'Valid: {100 * valid_acc:.2f}% '\n",
    "                f'Test: {100 * test_acc:.2f}%')\n",
    "    \n",
    "    with open(\"scores.txt\", \"a\") as f:\n",
    "        f.write(kg_emb + ' best valid acc: ' + str(best_valid_acc) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdb_TransR_ep_50_dim_200',\n",
       " 'imdb_TransH_ep_25_dim_200',\n",
       " 'imdb_RotatE_ep_25_dim_200',\n",
       " 'imdb_DistMult_ep_25_dim_200',\n",
       " 'imdb_TransH_ep_50_dim_200',\n",
       " 'imdb_TransR_ep_25_dim_200',\n",
       " 'imdb_RotatE_ep_50_dim_200',\n",
       " 'imdb_DistMult_ep_50_dim_200']"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_emb_models = [] \n",
    "for model in os.listdir('./Models_pykeen'):\n",
    "    if 'ep' in model:\n",
    "        kg_emb_models.append(model)\n",
    "kg_emb_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch: 01, Loss: 1.1074, Train: 39.75%, Valid: 39.50% Test: 36.43%\n",
      "Epoch: 02, Loss: 1.6132, Train: 39.75%, Valid: 39.50% Test: 36.43%\n",
      "Epoch: 03, Loss: 1.1192, Train: 35.25%, Valid: 39.50% Test: 36.23%\n",
      "Epoch: 04, Loss: 1.1734, Train: 35.25%, Valid: 39.50% Test: 36.23%\n",
      "Epoch: 05, Loss: 1.2742, Train: 36.50%, Valid: 22.75% Test: 28.72%\n",
      "Epoch: 06, Loss: 1.2396, Train: 57.00%, Valid: 30.75% Test: 37.18%\n",
      "Epoch: 07, Loss: 1.0959, Train: 73.50%, Valid: 46.25% Test: 41.55%\n",
      "Epoch: 08, Loss: 0.9894, Train: 39.75%, Valid: 39.50% Test: 36.43%\n",
      "Epoch: 09, Loss: 1.0024, Train: 39.75%, Valid: 39.50% Test: 36.43%\n",
      "Epoch: 10, Loss: 1.0439, Train: 39.75%, Valid: 39.50% Test: 36.43%\n",
      "Epoch: 11, Loss: 1.0188, Train: 41.50%, Valid: 39.50% Test: 36.46%\n",
      "Epoch: 12, Loss: 0.9359, Train: 69.25%, Valid: 42.00% Test: 39.02%\n",
      "Epoch: 13, Loss: 0.8500, Train: 92.00%, Valid: 51.75% Test: 48.13%\n",
      "Epoch: 14, Loss: 0.7988, Train: 90.75%, Valid: 50.50% Test: 50.83%\n",
      "Epoch: 15, Loss: 0.7824, Train: 81.50%, Valid: 42.50% Test: 45.03%\n",
      "Epoch: 16, Loss: 0.7650, Train: 85.25%, Valid: 46.75% Test: 48.16%\n",
      "Epoch: 17, Loss: 0.7177, Train: 90.50%, Valid: 54.25% Test: 50.09%\n",
      "Epoch: 18, Loss: 0.6506, Train: 95.00%, Valid: 56.00% Test: 51.15%\n",
      "Epoch: 19, Loss: 0.5824, Train: 96.25%, Valid: 55.50% Test: 51.29%\n",
      "Epoch: 20, Loss: 0.5235, Train: 96.00%, Valid: 52.00% Test: 47.50%\n",
      "Epoch: 21, Loss: 0.4786, Train: 95.25%, Valid: 48.25% Test: 44.45%\n",
      "Epoch: 22, Loss: 0.4429, Train: 95.50%, Valid: 48.50% Test: 44.25%\n",
      "Epoch: 23, Loss: 0.4042, Train: 97.00%, Valid: 49.75% Test: 46.20%\n",
      "Epoch: 24, Loss: 0.3568, Train: 98.25%, Valid: 51.50% Test: 49.48%\n",
      "Epoch: 25, Loss: 0.3064, Train: 98.25%, Valid: 56.50% Test: 52.33%\n",
      "Epoch: 26, Loss: 0.2618, Train: 98.75%, Valid: 57.50% Test: 54.23%\n",
      "Epoch: 27, Loss: 0.2269, Train: 98.25%, Valid: 56.75% Test: 55.09%\n",
      "Epoch: 28, Loss: 0.1999, Train: 98.25%, Valid: 56.50% Test: 55.15%\n",
      "Epoch: 29, Loss: 0.1771, Train: 98.25%, Valid: 56.25% Test: 54.89%\n",
      "Epoch: 30, Loss: 0.1558, Train: 98.75%, Valid: 56.25% Test: 55.29%\n",
      "cpu\n",
      "Epoch: 01, Loss: 1.1642, Train: 39.75%, Valid: 39.50% Test: 36.43%\n",
      "Epoch: 02, Loss: 1.2493, Train: 41.50%, Valid: 39.50% Test: 36.80%\n",
      "Epoch: 03, Loss: 1.1883, Train: 36.75%, Valid: 39.50% Test: 36.26%\n",
      "Epoch: 04, Loss: 1.0717, Train: 25.00%, Valid: 21.00% Test: 27.37%\n",
      "Epoch: 05, Loss: 1.0835, Train: 25.00%, Valid: 21.00% Test: 27.34%\n",
      "Epoch: 06, Loss: 1.1120, Train: 46.25%, Valid: 23.75% Test: 29.27%\n",
      "Epoch: 07, Loss: 1.0361, Train: 53.25%, Valid: 41.25% Test: 37.49%\n",
      "Epoch: 08, Loss: 0.9910, Train: 69.50%, Valid: 48.75% Test: 44.22%\n",
      "Epoch: 09, Loss: 0.9809, Train: 68.25%, Valid: 42.25% Test: 38.38%\n",
      "Epoch: 10, Loss: 0.9633, Train: 50.25%, Valid: 40.25% Test: 36.72%\n",
      "Epoch: 11, Loss: 0.9345, Train: 56.00%, Valid: 40.75% Test: 36.98%\n",
      "Epoch: 12, Loss: 0.8829, Train: 72.50%, Valid: 43.25% Test: 39.48%\n",
      "Epoch: 13, Loss: 0.8229, Train: 93.50%, Valid: 54.50% Test: 49.51%\n",
      "Epoch: 14, Loss: 0.7804, Train: 95.25%, Valid: 55.25% Test: 52.88%\n",
      "Epoch: 15, Loss: 0.7498, Train: 93.75%, Valid: 51.25% Test: 52.67%\n",
      "Epoch: 16, Loss: 0.7072, Train: 96.50%, Valid: 54.75% Test: 54.14%\n",
      "Epoch: 17, Loss: 0.6476, Train: 97.50%, Valid: 55.50% Test: 51.98%\n",
      "Epoch: 18, Loss: 0.5856, Train: 94.50%, Valid: 52.00% Test: 47.35%\n",
      "Epoch: 19, Loss: 0.5340, Train: 92.50%, Valid: 49.00% Test: 45.37%\n",
      "Epoch: 20, Loss: 0.4898, Train: 92.25%, Valid: 51.75% Test: 45.74%\n",
      "Epoch: 21, Loss: 0.4444, Train: 94.50%, Valid: 52.75% Test: 47.30%\n",
      "Epoch: 22, Loss: 0.3955, Train: 96.00%, Valid: 55.75% Test: 50.83%\n",
      "Epoch: 23, Loss: 0.3457, Train: 97.75%, Valid: 56.75% Test: 53.34%\n",
      "Epoch: 24, Loss: 0.2996, Train: 98.25%, Valid: 56.50% Test: 55.00%\n",
      "Epoch: 25, Loss: 0.2611, Train: 98.50%, Valid: 56.25% Test: 54.31%\n",
      "Epoch: 26, Loss: 0.2295, Train: 98.50%, Valid: 55.75% Test: 54.37%\n",
      "Epoch: 27, Loss: 0.2004, Train: 98.75%, Valid: 56.25% Test: 54.40%\n",
      "Epoch: 28, Loss: 0.1716, Train: 98.75%, Valid: 56.25% Test: 54.17%\n",
      "Epoch: 29, Loss: 0.1448, Train: 99.25%, Valid: 57.00% Test: 54.23%\n",
      "Epoch: 30, Loss: 0.1220, Train: 99.25%, Valid: 57.00% Test: 54.31%\n"
     ]
    }
   ],
   "source": [
    "for model in kg_emb_models:\n",
    "    run_train_eval(data, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Evaluate without KG Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch: 01, Loss: 1.0969, Train: 62.25%, Valid: 46.50% Test: 41.14%\n",
      "Epoch: 02, Loss: 1.0792, Train: 68.75%, Valid: 45.50% Test: 41.29%\n",
      "Epoch: 03, Loss: 1.0598, Train: 70.50%, Valid: 46.75% Test: 41.75%\n",
      "Epoch: 04, Loss: 1.0369, Train: 71.50%, Valid: 47.25% Test: 42.32%\n",
      "Epoch: 05, Loss: 1.0100, Train: 73.00%, Valid: 47.75% Test: 42.58%\n",
      "Epoch: 06, Loss: 0.9789, Train: 74.25%, Valid: 48.25% Test: 42.64%\n",
      "Epoch: 07, Loss: 0.9435, Train: 75.25%, Valid: 48.50% Test: 43.36%\n",
      "Epoch: 08, Loss: 0.9039, Train: 77.75%, Valid: 48.00% Test: 43.67%\n",
      "Epoch: 09, Loss: 0.8602, Train: 80.75%, Valid: 48.50% Test: 44.13%\n",
      "Epoch: 10, Loss: 0.8128, Train: 84.75%, Valid: 49.25% Test: 44.42%\n",
      "Epoch: 11, Loss: 0.7619, Train: 87.50%, Valid: 50.50% Test: 45.26%\n",
      "Epoch: 12, Loss: 0.7083, Train: 89.50%, Valid: 51.00% Test: 46.18%\n",
      "Epoch: 13, Loss: 0.6526, Train: 93.00%, Valid: 52.50% Test: 47.38%\n",
      "Epoch: 14, Loss: 0.5955, Train: 93.50%, Valid: 53.50% Test: 48.39%\n",
      "Epoch: 15, Loss: 0.5379, Train: 94.25%, Valid: 54.50% Test: 49.34%\n",
      "Epoch: 16, Loss: 0.4808, Train: 95.50%, Valid: 54.75% Test: 50.40%\n",
      "Epoch: 17, Loss: 0.4252, Train: 96.50%, Valid: 55.00% Test: 50.86%\n",
      "Epoch: 18, Loss: 0.3720, Train: 97.50%, Valid: 57.50% Test: 51.81%\n",
      "Epoch: 19, Loss: 0.3221, Train: 98.25%, Valid: 59.00% Test: 52.50%\n",
      "Epoch: 20, Loss: 0.2760, Train: 98.00%, Valid: 58.00% Test: 53.68%\n",
      "Epoch: 21, Loss: 0.2344, Train: 98.50%, Valid: 57.00% Test: 54.89%\n",
      "Epoch: 22, Loss: 0.1976, Train: 98.75%, Valid: 58.00% Test: 55.35%\n",
      "Epoch: 23, Loss: 0.1657, Train: 99.00%, Valid: 58.25% Test: 55.26%\n",
      "Epoch: 24, Loss: 0.1386, Train: 99.00%, Valid: 58.00% Test: 55.52%\n",
      "Epoch: 25, Loss: 0.1160, Train: 99.00%, Valid: 59.00% Test: 55.46%\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Model Parameters\n",
    "emb_dim = 128\n",
    "hidden_dim = 128\n",
    "output_dim = num_classes\n",
    "num_layers = 3\n",
    "\n",
    "model = SplitGCN(data.metadata(), emb_dim, hidden_dim, output_dim, num_layers, device)\n",
    "\n",
    "model, data = model.to(device), data.to(device)\n",
    "\n",
    "print(next(model.parameters()).device)\n",
    "\n",
    "# Reset model parameters\n",
    "# model.reset_parameters()\n",
    "\n",
    "# Define hyperparameters\n",
    "num_epochs = 25\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "best_model = None\n",
    "best_valid_acc = 0\n",
    "\n",
    "for epoch in range(1, 1 + num_epochs):\n",
    "    # train model\n",
    "    loss = train(model, data, optimizer, loss_fn)\n",
    "    \n",
    "    # evaluate model \n",
    "    result = test(model, data)\n",
    "    \n",
    "    train_acc, valid_acc, test_acc = result\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "    print(f'Epoch: {epoch:02d}, '\n",
    "            f'Loss: {loss:.4f}, '\n",
    "            f'Train: {100 * train_acc:.2f}%, '\n",
    "            f'Valid: {100 * valid_acc:.2f}% '\n",
    "            f'Test: {100 * test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5899999737739563"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Evaluate with look-up embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relation embeddings: 2\n"
     ]
    }
   ],
   "source": [
    "num_clf_nodes = data[data.node_types[0]].x.shape[0]\n",
    "embs = [None, None]\n",
    "embs[0] = torch.Tensor([0,1])\n",
    "embs[1] = torch.Tensor([1,0])\n",
    "rel_embs_list = []\n",
    "for i in range (len(embs)):\n",
    "    rel_embs_list.append(torch.tensor(np.tile(embs[i], (num_clf_nodes, 1))))\n",
    "print('Number of relation embeddings:', len(rel_embs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch: 01, Loss: 1.0855, Train: 44.75%, Valid: 40.50% Test: 36.72%\n",
      "Epoch: 02, Loss: 1.0672, Train: 44.50%, Valid: 39.75% Test: 36.52%\n",
      "Epoch: 03, Loss: 1.0472, Train: 48.25%, Valid: 40.25% Test: 36.69%\n",
      "Epoch: 04, Loss: 1.0235, Train: 54.25%, Valid: 40.75% Test: 37.29%\n",
      "Epoch: 05, Loss: 0.9955, Train: 63.00%, Valid: 41.75% Test: 38.04%\n",
      "Epoch: 06, Loss: 0.9629, Train: 67.75%, Valid: 43.50% Test: 39.36%\n",
      "Epoch: 07, Loss: 0.9255, Train: 70.75%, Valid: 46.50% Test: 41.17%\n",
      "Epoch: 08, Loss: 0.8832, Train: 74.25%, Valid: 47.50% Test: 42.90%\n",
      "Epoch: 09, Loss: 0.8362, Train: 77.25%, Valid: 49.00% Test: 44.16%\n",
      "Epoch: 10, Loss: 0.7851, Train: 84.75%, Valid: 49.50% Test: 45.83%\n",
      "Epoch: 11, Loss: 0.7305, Train: 89.00%, Valid: 52.50% Test: 47.12%\n",
      "Epoch: 12, Loss: 0.6732, Train: 93.00%, Valid: 54.00% Test: 48.39%\n",
      "Epoch: 13, Loss: 0.6142, Train: 94.50%, Valid: 55.25% Test: 50.37%\n",
      "Epoch: 14, Loss: 0.5548, Train: 96.50%, Valid: 57.75% Test: 51.67%\n",
      "Epoch: 15, Loss: 0.4959, Train: 97.00%, Valid: 58.75% Test: 52.65%\n",
      "Epoch: 16, Loss: 0.4388, Train: 97.50%, Valid: 59.75% Test: 54.03%\n",
      "Epoch: 17, Loss: 0.3845, Train: 98.00%, Valid: 59.50% Test: 55.12%\n",
      "Epoch: 18, Loss: 0.3338, Train: 98.00%, Valid: 59.25% Test: 55.49%\n",
      "Epoch: 19, Loss: 0.2873, Train: 98.00%, Valid: 59.25% Test: 55.84%\n",
      "Epoch: 20, Loss: 0.2455, Train: 99.00%, Valid: 58.75% Test: 56.12%\n",
      "Epoch: 21, Loss: 0.2084, Train: 99.00%, Valid: 58.75% Test: 56.04%\n",
      "Epoch: 22, Loss: 0.1760, Train: 99.00%, Valid: 58.50% Test: 55.78%\n",
      "Epoch: 23, Loss: 0.1481, Train: 99.00%, Valid: 58.75% Test: 55.52%\n",
      "Epoch: 24, Loss: 0.1244, Train: 99.00%, Valid: 58.75% Test: 55.52%\n",
      "Epoch: 25, Loss: 0.1045, Train: 99.00%, Valid: 59.25% Test: 55.64%\n",
      "Epoch: 26, Loss: 0.0880, Train: 99.00%, Valid: 59.00% Test: 55.49%\n",
      "Epoch: 27, Loss: 0.0745, Train: 99.00%, Valid: 59.00% Test: 55.46%\n",
      "Epoch: 28, Loss: 0.0635, Train: 99.00%, Valid: 58.75% Test: 55.35%\n",
      "Epoch: 29, Loss: 0.0546, Train: 99.00%, Valid: 58.75% Test: 55.41%\n",
      "Epoch: 30, Loss: 0.0474, Train: 99.00%, Valid: 58.75% Test: 55.52%\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Model Parameters\n",
    "emb_dim = 128\n",
    "hidden_dim = 128\n",
    "output_dim = num_classes\n",
    "num_layers = 3\n",
    "\n",
    "model = SplitGCN(data.metadata(), emb_dim, hidden_dim, output_dim, num_layers, device, rel_embs_list)\n",
    "\n",
    "model, data = model.to(device), data.to(device)\n",
    "\n",
    "print(next(model.parameters()).device)\n",
    "\n",
    "# Reset model parameters\n",
    "# model.reset_parameters()\n",
    "\n",
    "# Define hyperparameters\n",
    "num_epochs = 30\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "best_model = None\n",
    "best_valid_acc = 0\n",
    "\n",
    "for epoch in range(1, 1 + num_epochs):\n",
    "    # train model\n",
    "    loss = train(model, data, optimizer, loss_fn, rel_embs_list)\n",
    "    \n",
    "    # evaluate model \n",
    "    result = test(model, data, rel_embs_list)\n",
    "    \n",
    "    train_acc, valid_acc, test_acc = result\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "    print(f'Epoch: {epoch:02d}, '\n",
    "            f'Loss: {loss:.4f}, '\n",
    "            f'Train: {100 * train_acc:.2f}%, '\n",
    "            f'Valid: {100 * valid_acc:.2f}% '\n",
    "            f'Test: {100 * test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5030792b3492f6b12d94f1f48beca3d8e59ec05fd59d0aaaa48e684281ed297"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
